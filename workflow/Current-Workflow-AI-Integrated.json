{
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "legal-analysis-upload",
        "responseMode": "responseNode",
        "options": {
          "allowedOrigins": "https://findingemail-0w07x.kinsta.page",
          "responseHeaders": {
            "entries": [
              {
                "name": "Access-Control-Allow-Origin",
                "value": "https://findingemail-0w07x.kinsta.page"
              },
              {
                "name": "Access-Control-Allow-Methods",
                "value": "POST, OPTIONS"
              },
              {
                "name": "Access-Control-Allow-Headers",
                "value": "Content-Type"
              }
            ]
          }
        }
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        -176,
        208
      ],
      "id": "3b073c31-a949-442e-a4e7-efe0a5317d09",
      "name": "Webhook",
      "webhookId": "6d7fe8eb-8786-4526-88ad-5cd8fbf1c6c6"
    },
    {
      "parameters": {
        "jsCode": "// Step 2: Data Structure Validation for Legal Document Analysis\n// This code validates incoming form data and ensures it matches our expected schema\n\n// First, let's extract the incoming data from the webhook\n// Based on Step 1 findings, form data is nested in the 'body' property\nconst incomingData = $input.first().json;\n\n// Initialize our validation results object\nconst validationResult = {\n  isValid: false,\n  errors: [],\n  validatedData: null,\n  processingTimestamp: new Date().toISOString()\n};\n\n// Check if the basic structure exists\n// We expect data to be in incomingData.body based on Step 1 findings\nif (!incomingData.body) {\n  validationResult.errors.push(\"Missing form data structure. Expected 'body' property not found.\");\n  return [{ json: validationResult }];\n}\n\nconst formData = incomingData.body;\n\n// Validate required fields according to Form_Output_Documentation.md\n// Required fields: clientName and attorneyName\nconst requiredFields = ['clientName', 'attorneyName'];\n\nfor (const field of requiredFields) {\n  if (!formData[field]) {\n    validationResult.errors.push(`Missing required field: ${field}`);\n  } else if (typeof formData[field] !== 'string') {\n    validationResult.errors.push(`Field ${field} must be a string`);\n  } else if (formData[field].trim().length === 0) {\n    validationResult.errors.push(`Field ${field} cannot be empty`);\n  }\n}\n\n// Validate optional field (caseReference) if present\nif (formData.caseReference && typeof formData.caseReference !== 'string') {\n  validationResult.errors.push(\"Field caseReference must be a string\");\n}\n\n// If we have errors, return them immediately\nif (validationResult.errors.length > 0) {\n  validationResult.isValid = false;\n  return [{ json: validationResult }];\n}\n\n// If validation passes, prepare clean data structure\nconst cleanData = {\n  clientName: formData.clientName.trim(),\n  caseReference: formData.caseReference ? formData.caseReference.trim() : '',\n  attorneyName: formData.attorneyName.trim()\n};\n\n// Set success status and return validated data\nvalidationResult.isValid = true;\nvalidationResult.validatedData = cleanData;\nvalidationResult.message = \"Form data validation successful\";\n\nreturn [{ json: validationResult }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        48,
        208
      ],
      "id": "0e84bbe7-4653-4251-86cc-2b8b01bf8a91",
      "name": "Validate Form Data"
    },
    {
      "parameters": {
        "jsCode": "// Step 3: Intake Form Data Processing\n// This node structures the validated intake form data into a standardized format\n// Based on the established pattern from the proposed workflow's caseInfo structure\n\nconst validationData = $input.first().json;\n\n// Only process if validation was successful\nif (!validationData.isValid) {\n  // Pass through validation errors unchanged\n  return [{ json: validationData }];\n}\n\n// Extract the clean validated data\nconst validatedFields = validationData.validatedData;\n\n// Structure the intake form data into standardized JSON format\n// Following the caseInfo pattern from the proposed workflow\nconst structuredIntakeData = {\n  caseInfo: {\n    clientName: validatedFields.clientName,\n    caseReference: validatedFields.caseReference || `CASE-${new Date().toISOString().split('T')[0]}`,\n    attorneyName: validatedFields.attorneyName,\n    processingDate: new Date().toISOString()\n  },\n  dataProcessingStatus: {\n    intakeFormProcessed: true,\n    structuredAt: new Date().toISOString(),\n    dataQuality: 'validated'\n  },\n  nextStage: 'document-processing'\n};\n\n// Return the structured data with success metadata\nreturn [{\n  json: {\n    isValid: true,\n    structuredData: structuredIntakeData,\n    processingTimestamp: validationData.processingTimestamp,\n    message: 'Intake form data successfully structured for processing'\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        256,
        208
      ],
      "id": "813dc0b0-0630-4cf1-bb99-5350e186495d",
      "name": "Structure Intake Data"
    },
    {
      "parameters": {
        "jsCode": "// Step 4: Binary Data Reception and File Cataloging\n// This node extracts binary files from webhook and creates file catalog\n\n// Get the structured intake data from previous step\nconst intakeData = $input.first().json;\n\n// Get the original webhook data that contains binary files\n// We need to look back at the webhook node's output\nconst webhookItems = $items(\"Webhook\");\nconst binaryData = webhookItems[0].binary || {};\n\n// Initialize file processing results\nconst fileProcessingResult = {\n  isValid: true,\n  errors: [],\n  filesCatalog: {\n    intakeForm: null,\n    caseDocuments: [],\n    totalFileSize: 0,\n    fileCount: 0\n  },\n  structuredData: intakeData.structuredData || {},\n  processingTimestamp: new Date().toISOString()\n};\n\n// Check if we have any binary data\nif (!binaryData || Object.keys(binaryData).length === 0) {\n  fileProcessingResult.isValid = false;\n  fileProcessingResult.errors.push(\"No files were uploaded with the form submission\");\n  return [{ json: fileProcessingResult }];\n}\n\n// Define allowed file types\nconst allowedMimeTypes = [\n  'application/pdf',\n  'application/vnd.openxmlformats-officedocument.wordprocessingml.document', // .docx\n  'application/msword', // .doc\n  'text/plain' // .txt\n];\n\n// Size limit: 100MB total\nconst MAX_TOTAL_SIZE = 100 * 1024 * 1024; // 100MB in bytes\nlet totalSize = 0;\n\n// Process intake form (required)\nif (binaryData.intakeForm) {\n  const intakeFile = binaryData.intakeForm;\n  \n  // Validate file type\n  if (!allowedMimeTypes.includes(intakeFile.mimeType)) {\n    fileProcessingResult.errors.push(`Intake form has invalid file type: ${intakeFile.mimeType}`);\n  }\n  \n  // Extract metadata with proper size handling\n  const sizeInBytes = typeof intakeFile.fileSize === 'number'\n    ? intakeFile.fileSize\n    : intakeFile.data ? intakeFile.data.length : 0;\n  \n  fileProcessingResult.filesCatalog.intakeForm = {\n    fileName: intakeFile.fileName,\n    fileSize: sizeInBytes,\n    mimeType: intakeFile.mimeType,\n    uploadedAt: new Date().toISOString()\n  };\n  \n  totalSize += sizeInBytes;\n  fileProcessingResult.filesCatalog.fileCount++;\n} else {\n  fileProcessingResult.isValid = false;\n  fileProcessingResult.errors.push(\"Required intake form is missing\");\n}\n\n// Process case documents\nlet documentIndex = 0;\nlet hasAtLeastOneDocument = false;\n\nwhile (binaryData[`caseDocument${documentIndex}`]) {\n  const docFile = binaryData[`caseDocument${documentIndex}`];\n  hasAtLeastOneDocument = true;\n  \n  // Validate file type\n  if (!allowedMimeTypes.includes(docFile.mimeType)) {\n    fileProcessingResult.errors.push(`Case document ${documentIndex} has invalid file type: ${docFile.mimeType}`);\n  }\n  \n  // Extract metadata with proper size handling\n  const sizeInBytes = typeof docFile.fileSize === 'number'\n    ? docFile.fileSize\n    : docFile.data ? docFile.data.length : 0;\n  \n  // Add to catalog\n  fileProcessingResult.filesCatalog.caseDocuments.push({\n    index: documentIndex,\n    fileName: docFile.fileName,\n    fileSize: sizeInBytes,\n    mimeType: docFile.mimeType,\n    uploadedAt: new Date().toISOString()\n  });\n  \n  totalSize += sizeInBytes;\n  fileProcessingResult.filesCatalog.fileCount++;\n  documentIndex++;\n}\n\n// Validate at least one case document\nif (!hasAtLeastOneDocument) {\n  fileProcessingResult.isValid = false;\n  fileProcessingResult.errors.push(\"At least one case document is required\");\n}\n\n// Check total size limit\nfileProcessingResult.filesCatalog.totalFileSize = totalSize;\nif (totalSize > MAX_TOTAL_SIZE) {\n  fileProcessingResult.isValid = false;\n  fileProcessingResult.errors.push(`Total file size (${(totalSize / 1024 / 1024).toFixed(2)}MB) exceeds 100MB limit`);\n}\n\n// If there are validation errors, mark as invalid\nif (fileProcessingResult.errors.length > 0) {\n  fileProcessingResult.isValid = false;\n}\n\n// Merge with intake data if valid\nif (fileProcessingResult.isValid && intakeData.isValid) {\n  fileProcessingResult.message = \"Files successfully received and cataloged\";\n  fileProcessingResult.structuredData.filesCatalog = fileProcessingResult.filesCatalog;\n  fileProcessingResult.structuredData.dataProcessingStatus.filesProcessed = true;\n  fileProcessingResult.structuredData.dataProcessingStatus.filesProcessedAt = new Date().toISOString();\n  fileProcessingResult.structuredData.nextStage = 'document-categorization';\n}\n\n// Pass binary data forward for downstream processing\nreturn [{\n  json: fileProcessingResult,\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        672,
        208
      ],
      "id": "4f9e8a1b-2c5d-4e8f-9a1b-3c4d5e6f7a8b",
      "name": "Extract Binary Files"
    },
    {
      "parameters": {
        "jsCode": "// Step 5: Document Categorization Router\n// This node creates separate processing branches for intake forms and case documents\n// Based on the file catalog from Step 4, it prepares data for parallel processing\n\n// Get the file processing result from Step 4\nconst step4Data = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Initialize the routing results\nconst routingResults = {\n  isValid: true,\n  errors: [],\n  branches: [],\n  processingTimestamp: new Date().toISOString()\n};\n\n// Validate that we have the necessary data from Step 4\nif (!step4Data || !step4Data.isValid) {\n  routingResults.isValid = false;\n  routingResults.errors.push(\"Invalid data received from Step 4 - file processing failed\");\n  return [{ json: routingResults }];\n}\n\n// Extract the structured data and file catalog\nconst structuredData = step4Data.structuredData || {};\nconst filesCatalog = structuredData.filesCatalog || step4Data.filesCatalog;\n\nif (!filesCatalog) {\n  routingResults.isValid = false;\n  routingResults.errors.push(\"File catalog is missing from Step 4 data\");\n  return [{ json: routingResults }];\n}\n\n// Prepare output items for the Switch node\nconst outputItems = [];\n\n// Branch 1: Intake Form Processing\nif (filesCatalog.intakeForm) {\n  const intakeFormBranch = {\n    documentType: \"intakeForm\",\n    caseInfo: structuredData.caseInfo || {},\n    intakeFormData: {\n      fileName: filesCatalog.intakeForm.fileName,\n      fileSize: filesCatalog.intakeForm.fileSize,\n      mimeType: filesCatalog.intakeForm.mimeType,\n      uploadedAt: filesCatalog.intakeForm.uploadedAt,\n      binaryDataReference: \"intakeForm\"\n    },\n    processingMetadata: {\n      branch: \"intake-analysis\",\n      documentCount: 1,\n      processingStartedAt: new Date().toISOString(),\n      priority: \"high\"\n    },\n    originalData: {\n      step4Result: step4Data,\n      totalFileCount: filesCatalog.fileCount,\n      totalFileSize: filesCatalog.totalFileSize\n    }\n  };\n  \n  outputItems.push({\n    json: intakeFormBranch,\n    binary: { intakeForm: binaryData.intakeForm }\n  });\n  \n  routingResults.branches.push(\"intake-analysis\");\n} else {\n  routingResults.isValid = false;\n  routingResults.errors.push(\"Intake form is required but missing from file catalog\");\n}\n\n// Branch 2: Case Documents Processing\nif (filesCatalog.caseDocuments && filesCatalog.caseDocuments.length > 0) {\n  // Prepare binary data object for case documents\n  const caseDocumentsBinary = {};\n  \n  // Add each case document to binary data\n  filesCatalog.caseDocuments.forEach((doc, index) => {\n    const binaryKey = `caseDocument${doc.index}`;\n    if (binaryData[binaryKey]) {\n      caseDocumentsBinary[binaryKey] = binaryData[binaryKey];\n    }\n  });\n  \n  const caseDocumentsBranch = {\n    documentType: \"caseDocuments\",\n    caseInfo: structuredData.caseInfo || {},\n    caseDocumentsData: {\n      documentCount: filesCatalog.caseDocuments.length,\n      totalSize: filesCatalog.caseDocuments.reduce((total, doc) => total + doc.fileSize, 0),\n      documents: filesCatalog.caseDocuments.map(doc => ({\n        index: doc.index,\n        fileName: doc.fileName,\n        fileSize: doc.fileSize,\n        mimeType: doc.mimeType,\n        uploadedAt: doc.uploadedAt,\n        binaryDataReference: `caseDocument${doc.index}`\n      }))\n    },\n    processingMetadata: {\n      branch: \"case-documents-analysis\",\n      documentCount: filesCatalog.caseDocuments.length,\n      processingStartedAt: new Date().toISOString(),\n      priority: \"normal\"\n    },\n    originalData: {\n      step4Result: step4Data,\n      totalFileCount: filesCatalog.fileCount,\n      totalFileSize: filesCatalog.totalFileSize\n    }\n  };\n  \n  outputItems.push({\n    json: caseDocumentsBranch,\n    binary: caseDocumentsBinary\n  });\n  \n  routingResults.branches.push(\"case-documents-analysis\");\n} else {\n  routingResults.isValid = false;\n  routingResults.errors.push(\"At least one case document is required but none found in file catalog\");\n}\n\n// If we have validation errors, return error response\nif (routingResults.errors.length > 0) {\n  routingResults.isValid = false;\n  return [{ json: routingResults }];\n}\n\n// Return all output items for the Switch node\nreturn outputItems;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        880,
        208
      ],
      "id": "5a9b8c2d-3e4f-5g6h-7i8j-9k0l1m2n3o4p",
      "name": "Categorize Documents"
    },
    {
      "parameters": {
        "conditions": {
          "string": [
            {
              "value1": "={{ $json.documentType }}",
              "value2": "intakeForm"
            }
          ]
        }
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        1088,
        128
      ],
      "id": "6b0c9d3e-4f5g-6h7i-8j9k-0l1m2n3o4p5q",
      "name": "Route Intake Form"
    },
    {
      "parameters": {
        "conditions": {
          "string": [
            {
              "value1": "={{ $json.documentType }}",
              "value2": "caseDocuments"
            }
          ]
        }
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        1088,
        288
      ],
      "id": "7c1d0e4f-5g6h-7i8j-9k0l-1m2n3o4p5q6r",
      "name": "Route Case Documents"
    },
    {
      "parameters": {
        "jsCode": "// AI Intake Form Request Preparation\n// This node prepares the request for live AI processing using Anthropic Claude\n// Replaces the first part of the simulated Extract Intake Form Data node\n\nconst inputData = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Validate input structure\nif (!inputData || inputData.documentType !== \"intakeForm\") {\n  throw new Error(\"Invalid input: Expected intakeForm document type\");\n}\n\n// Extract binary data\nconst binaryKey = inputData.intakeFormData.binaryDataReference;\nconst documentBinary = binaryData[binaryKey];\n\nif (!documentBinary) {\n  throw new Error(`Binary data not found for key: ${binaryKey}`);\n}\n\n// Note: In production, implement actual text extraction from PDF/DOCX\n// For now, assume text extraction happens upstream or use simulated text\nconst documentText = documentBinary.data ? documentBinary.data.toString('utf8') : `\nLEGAL INTAKE FORM\n\nClient Information:\nName: John Smith\nPhone: (555) 123-4567\nEmail: john.smith@email.com\nAddress: 123 Main Street, City, ST 12345\n\nCase Information:\nCase Reference: CASE-2025-001\nMatter Type: Personal Injury\nDate of Incident: January 15, 2025\nBrief Description: Motor vehicle accident at intersection of Main St and Oak Ave. Client was injured when defendant ran red light.\n\nAttorney Information:\nAssigned Attorney: Sarah Johnson, Esq.\nBar Number: 12345\nSpecialty: Personal Injury Law\n\nAdditional Notes:\nClient reported back and neck pain. Medical treatment ongoing at City Hospital.\n`;\n\n// Prepare AI request payload for Anthropic Claude\nconst aiRequestPayload = {\n  model: \"claude-3-sonnet-20240229\",\n  max_tokens: 1000,\n  temperature: 0.1,\n  messages: [{\n    role: \"user\",\n    content: `You are an expert legal data extraction assistant. Your task is to extract information from the provided intake form document and format it as a valid JSON object.\n\nThe document content is as follows:\n${documentText}\n\nPlease extract the following information and structure it according to the JSON schema provided below. Ensure all fields are correctly populated and that the JSON is perfectly formatted.\n\n**JSON Schema:**\n{\n  \"clientInfo\": {\n    \"clientName\": \"string\",\n    \"contactInfo\": \"string\"\n  },\n  \"caseInfo\": {\n    \"caseReference\": \"string\",\n    \"summary\": \"string\"\n  },\n  \"attorneyInfo\": {\n    \"attorneyName\": \"string\"\n  }\n}\n\nReturn ONLY the valid JSON object.`\n  }]\n};\n\nreturn [{\n  json: {\n    ...inputData,\n    aiRequestPayload: aiRequestPayload,\n    processingMetadata: {\n      nodeType: \"ai-request-preparation\",\n      requestPreparedAt: new Date().toISOString(),\n      documentTextLength: documentText.length\n    }\n  },\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1296,
        128
      ],
      "id": "ai-prepare-intake-001-2025",
      "name": "Prepare AI Intake Request"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.anthropic.com/v1/messages",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "httpHeaderAuth",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify($json.aiRequestPayload) }}",
        "options": {
          "timeout": 30000,
          "retry": {
            "enabled": true,
            "maxTries": 3,
            "waitBetween": 2000
          }
        },
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            },
            {
              "name": "anthropic-version",
              "value": "2023-06-01"
            }
          ]
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        1504,
        128
      ],
      "id": "ai-execute-intake-001-2025",
      "name": "Execute AI Intake Request",
      "credentials": {
        "httpHeaderAuth": {
          "id": "anthropic-api-legal-workflow",
          "name": "Anthropic API - Legal Workflow"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Parse AI Response and Validate\n// This node processes the response from Anthropic Claude and validates the extraction\n\nconst aiResponse = $input.first().json;\nconst originalData = $items(\"Prepare AI Intake Request\")[0].json;\n\n// Initialize processing result\nconst processingResult = {\n  isValid: true,\n  errors: [],\n  extractedData: null,\n  processingTimestamp: new Date().toISOString(),\n  extractionMetadata: {\n    nodeType: \"ai-response-parser\",\n    processingBranch: \"intake-analysis\",\n    aiModel: \"claude-3-sonnet\"\n  }\n};\n\ntry {\n  // Check for API errors first\n  if (aiResponse.error) {\n    throw new Error(`API Error: ${aiResponse.error.message || 'Unknown API error'}`);\n  }\n  \n  // Extract AI response content\n  if (!aiResponse.content || !Array.isArray(aiResponse.content) || aiResponse.content.length === 0) {\n    throw new Error(\"Invalid API response: Missing content array\");\n  }\n  \n  const aiContent = aiResponse.content[0].text;\n  if (!aiContent) {\n    throw new Error(\"Invalid API response: Missing text content\");\n  }\n  \n  // Parse JSON response\n  let extractedData;\n  try {\n    extractedData = JSON.parse(aiContent);\n  } catch (parseError) {\n    throw new Error(`JSON parsing failed: ${parseError.message}`);\n  }\n  \n  // Validate required fields\n  const requiredFields = [\n    'clientInfo.clientName',\n    'caseInfo.caseReference',\n    'attorneyInfo.attorneyName'\n  ];\n  \n  for (const field of requiredFields) {\n    const fieldPath = field.split('.');\n    let value = extractedData;\n    for (const key of fieldPath) {\n      value = value?.[key];\n    }\n    if (!value || typeof value !== 'string' || value.trim().length === 0) {\n      throw new Error(`Missing or invalid required field: ${field}`);\n    }\n  }\n  \n  // Set successful extraction result\n  processingResult.extractedData = extractedData;\n  processingResult.extractionMetadata.fieldsExtracted = Object.keys(extractedData).length;\n  processingResult.extractionMetadata.tokenUsage = aiResponse.usage || {};\n  processingResult.message = \"Intake form data successfully extracted using live AI\";\n  \n  // Preserve original routing data\n  processingResult.originalRoutingData = {\n    documentType: originalData.documentType,\n    caseInfo: originalData.caseInfo,\n    intakeFormData: originalData.intakeFormData,\n    processingMetadata: originalData.processingMetadata\n  };\n  \n} catch (error) {\n  processingResult.isValid = false;\n  processingResult.errors.push(`AI processing failed: ${error.message}`);\n  processingResult.extractionMetadata.rawResponse = aiResponse;\n  \n  // Implement fallback strategy\n  console.error('AI processing failed, using fallback', error.message);\n  \n  // Return simulated response structure as fallback\n  processingResult.extractedData = {\n    clientInfo: {\n      clientName: \"AI_PROCESSING_FAILED\",\n      contactInfo: \"Please review manually\"\n    },\n    caseInfo: {\n      caseReference: `MANUAL_REVIEW_${new Date().toISOString().split('T')[0]}`,\n      summary: \"AI processing failed - manual review required\"\n    },\n    attorneyInfo: {\n      attorneyName: \"REQUIRES_MANUAL_ENTRY\"\n    }\n  };\n  \n  processingResult.isValid = true;\n  processingResult.fallbackUsed = true;\n  processingResult.message = \"Fallback response used due to AI processing failure\";\n}\n\nreturn [{\n  json: processingResult,\n  binary: $items(\"Prepare AI Intake Request\")[0].binary\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1712,
        128
      ],
      "id": "ai-parse-intake-001-2025",
      "name": "Parse AI Intake Response"
    },
    {
      "parameters": {
        "jsCode": "// Parse & Enrich Intake Data Node\n// This node receives the AI output from Parse AI Intake Response,\n// validates the structure, and enriches it with processing metadata\n\n// Get the input data from the Parse AI Intake Response node\nconst inputData = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Initialize the processing result\nconst processingResult = {\n  isValid: true,\n  errors: [],\n  enrichedData: null,\n  processingTimestamp: new Date().toISOString(),\n  processingMetadata: {\n    nodeType: \"parse-enrich\",\n    processingBranch: \"intake-analysis\",\n    validationPassed: false,\n    enrichmentApplied: false\n  }\n};\n\n// Validate that we received valid extraction data\nif (!inputData || !inputData.isValid) {\n  processingResult.isValid = false;\n  processingResult.errors.push(\"Invalid input: Previous AI extraction step failed\");\n  if (inputData && inputData.errors) {\n    processingResult.errors = processingResult.errors.concat(inputData.errors);\n  }\n  return [{ json: processingResult, binary: binaryData }];\n}\n\n// Extract the JSON data from the extraction result\nlet extractedJsonData;\ntry {\n  if (inputData.extractedData) {\n    // Data is already parsed as an object from the extraction step\n    extractedJsonData = inputData.extractedData;\n  } else {\n    processingResult.isValid = false;\n    processingResult.errors.push(\"No extracted data found in input\");\n    return [{ json: processingResult, binary: binaryData }];\n  }\n} catch (error) {\n  processingResult.isValid = false;\n  processingResult.errors.push(`Failed to parse JSON: ${error.message}`);\n  return [{ json: processingResult, binary: binaryData }];\n}\n\n// Validate the presence of required top-level keys\nconst requiredKeys = ['clientInfo', 'caseInfo', 'attorneyInfo'];\nconst missingKeys = [];\n\nfor (const key of requiredKeys) {\n  if (!extractedJsonData[key] || typeof extractedJsonData[key] !== 'object') {\n    missingKeys.push(key);\n  }\n}\n\nif (missingKeys.length > 0) {\n  processingResult.isValid = false;\n  processingResult.errors.push(`Missing or invalid required keys: ${missingKeys.join(', ')}`);\n  return [{ json: processingResult, binary: binaryData }];\n}\n\n// Validation passed - proceed with enrichment\nprocessingResult.processingMetadata.validationPassed = true;\n\n// Create enriched data by adding processing metadata\nconst enrichedJsonData = {\n  ...extractedJsonData,\n  processing: {\n    extractionStatus: \"validated\",\n    parsedAt: new Date().toISOString(),\n    parserVersion: \"1.0.0\",\n    validatedKeys: requiredKeys,\n    originalExtractionTimestamp: inputData.processingTimestamp,\n    extractionMetadata: inputData.extractionMetadata || {},\n    fallbackUsed: inputData.fallbackUsed || false\n  }\n};\n\n// Set successful processing result\nprocessingResult.enrichedData = enrichedJsonData;\nprocessingResult.processingMetadata.enrichmentApplied = true;\nprocessingResult.message = \"Intake data successfully parsed, validated, and enriched\";\n\n// Preserve original routing and extraction data for downstream processing\nprocessingResult.originalData = {\n  extractionResult: inputData,\n  routingData: inputData.originalRoutingData || {}\n};\n\n// Return the enriched result\nreturn [{\n  json: processingResult,\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1920,
        128
      ],
      "id": "parse-enrich-intake-001-2025",
      "name": "Parse & Enrich Intake Data"
    },
    {
      "parameters": {
        "jsCode": "// Validate Intake Extraction Node\n// This node performs final quality check on the enriched intake data\n// and adds final validation timestamp if processing status is validated\n\n// Get the input data from the Parse & Enrich Intake Data node\nconst inputData = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Initialize the validation result\nconst validationResult = {\n  isValid: true,\n  errors: [],\n  validatedData: null,\n  processingTimestamp: new Date().toISOString(),\n  validationMetadata: {\n    nodeType: \"final-validation\",\n    processingBranch: \"intake-analysis\",\n    finalValidationApplied: false\n  }\n};\n\n// Validate that we received valid processing data\nif (!inputData || !inputData.isValid) {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"Invalid input: Previous processing step failed\");\n  if (inputData && inputData.errors) {\n    validationResult.errors = validationResult.errors.concat(inputData.errors);\n  }\n  return [{ json: validationResult, binary: binaryData }];\n}\n\n// Extract the enriched data\nconst enrichedData = inputData.enrichedData;\nif (!enrichedData) {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"No enriched data found in input\");\n  return [{ json: validationResult, binary: binaryData }];\n}\n\n// Validate Processing Status: Check if processing.extractionStatus === \"validated\"\nif (!enrichedData.processing || enrichedData.processing.extractionStatus !== \"validated\") {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"Final validation failed: Extraction status is not 'validated'\");\n  validationResult.actualStatus = enrichedData.processing ? enrichedData.processing.extractionStatus : \"missing\";\n  return [{ json: validationResult, binary: binaryData }];\n}\n\n// Finalize Output: Add finalValidationTimestamp to the processing object\nconst finalValidatedData = {\n  ...enrichedData,\n  processing: {\n    ...enrichedData.processing,\n    finalValidationTimestamp: new Date().toISOString(),\n    finalValidationStatus: \"completed\"\n  }\n};\n\n// Set successful validation result\nvalidationResult.validatedData = finalValidatedData;\nvalidationResult.validationMetadata.finalValidationApplied = true;\nvalidationResult.message = \"Final validation completed successfully - intake extraction validated\";\n\n// Preserve original processing and routing data\nvalidationResult.originalData = {\n  processingResult: inputData,\n  extractionData: inputData.originalData || {}\n};\n\n// Return the final validated result\nreturn [{\n  json: validationResult,\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2128,
        128
      ],
      "id": "validate-intake-final-001-2025",
      "name": "Validate Intake Extraction"
    },
    {
      "parameters": {
        "jsCode": "// Prepare Case Documents Node\n// This node receives case documents data and simulates text extraction\n// from multiple files, aggregating them into a single text string\n\n// Get the input data from the Route Case Documents node\nconst inputData = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Initialize preparation results\nconst preparationResult = {\n  isValid: true,\n  errors: [],\n  aggregatedDocumentsText: null,\n  processingTimestamp: new Date().toISOString(),\n  preparationMetadata: {\n    nodeType: \"prepare-documents\",\n    processingBranch: \"case-documents-analysis\",\n    simulatedProcessing: true\n  }\n};\n\n// Validate input data structure\nif (!inputData || inputData.documentType !== \"caseDocuments\") {\n  preparationResult.isValid = false;\n  preparationResult.errors.push(\"Invalid input: Expected caseDocuments document type\");\n  return [{ json: preparationResult }];\n}\n\n// Check for case documents data\nif (!inputData.caseDocumentsData || !inputData.caseDocumentsData.documents) {\n  preparationResult.isValid = false;\n  preparationResult.errors.push(\"Missing caseDocumentsData.documents array\");\n  return [{ json: preparationResult }];\n}\n\nconst documents = inputData.caseDocumentsData.documents;\nif (!Array.isArray(documents) || documents.length === 0) {\n  preparationResult.isValid = false;\n  preparationResult.errors.push(\"caseDocumentsData.documents must be a non-empty array\");\n  return [{ json: preparationResult }];\n}\n\n// SIMULATION: Text Extraction from Multiple Documents\n// Since we cannot yet read text from multiple files, we simulate this step\nconst aggregatedTextParts = [];\nconst documentSeparator = \"\\n\\n--- DOCUMENT SEPARATOR ---\\n\\n\";\n\n// Iterate through each document and create simulated text content\ndocuments.forEach((doc, index) => {\n  // Create distinct placeholder text for each document\n  const documentNumber = index + 1;\n  const simulatedText = `DOCUMENT ${documentNumber}: ${doc.fileName || `Document_${documentNumber}`}\n\nFile Information:\n- File Name: ${doc.fileName || `Document_${documentNumber}.pdf`}\n- File Size: ${doc.fileSize ? (doc.fileSize / 1024).toFixed(2) + ' KB' : 'Unknown'}\n- MIME Type: ${doc.mimeType || 'application/pdf'}\n- Upload Time: ${doc.uploadedAt || new Date().toISOString()}\n\nSimulated Document Content:\nThis is simulated content for document ${documentNumber}. In a production environment, this would contain the actual extracted text from the ${doc.mimeType || 'PDF'} file. The document appears to be a legal document related to the case proceedings.\n\nKey sections that might be found in this document:\n- Case details and background information\n- Legal arguments and supporting evidence\n- Timeline of events relevant to the case\n- Party information and contact details\n- Court filings and procedural documents\n- Evidence and supporting documentation\n\nThis simulated text represents what would typically be extracted from document ${documentNumber} in the case file. The actual content would provide detailed legal information relevant to the case analysis and would be used for downstream AI processing and analysis.\n\nEnd of simulated content for document ${documentNumber}.`;\n  \n  aggregatedTextParts.push(simulatedText);\n});\n\n// Combine all document texts with separators\nconst aggregatedText = aggregatedTextParts.join(documentSeparator);\n\n// Add final metadata footer\nconst metadataFooter = `\\n\\n--- END OF DOCUMENTS ---\\n\\nDocument Processing Summary:\n- Total Documents Processed: ${documents.length}\n- Total Aggregated Text Length: ${aggregatedText.length} characters\n- Processing Method: Simulated text extraction\n- Processing Timestamp: ${new Date().toISOString()}\n- Branch: case-documents-analysis`;\n\nconst finalAggregatedText = aggregatedText + metadataFooter;\n\n// Set successful preparation result\npreparationResult.aggregatedDocumentsText = finalAggregatedText;\npreparationResult.preparationMetadata.documentsProcessed = documents.length;\npreparationResult.preparationMetadata.totalTextLength = finalAggregatedText.length;\npreparationResult.preparationMetadata.processingMethod = \"simulated-extraction\";\npreparationResult.message = `Successfully prepared ${documents.length} case documents for analysis`;\n\n// Preserve original routing data for downstream processing\npreparationResult.originalRoutingData = {\n  documentType: inputData.documentType,\n  caseInfo: inputData.caseInfo,\n  caseDocumentsData: inputData.caseDocumentsData,\n  processingMetadata: inputData.processingMetadata\n};\n\n// Return the preparation result with aggregated text\nreturn [{\n  json: preparationResult,\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1296,
        288
      ],
      "id": "prepare-case-docs-001-2025",
      "name": "Prepare Case Documents"
    },
    {
      "parameters": {
        "jsCode": "// AI Case Documents Request Preparation\n// This node prepares the request for live AI processing using Anthropic Claude Opus\n// Replaces the simulated Analyze Case Documents node with live AI\n\nconst inputData = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Validate input data structure\nif (!inputData || !inputData.isValid || !inputData.aggregatedDocumentsText) {\n  throw new Error(\"Invalid input: Missing aggregated documents text\");\n}\n\nconst documentsText = inputData.aggregatedDocumentsText;\n\n// Prepare AI request payload for Anthropic Claude Opus (better for complex analysis)\nconst aiRequestPayload = {\n  model: \"claude-3-opus-20240229\",\n  max_tokens: 4000,\n  temperature: 0.1,\n  messages: [{\n    role: \"user\",\n    content: `You are a specialized legal analysis AI. You will be given a collection of case documents. Your task is to perform a comprehensive analysis across all provided documents and extract key legal information.\n\nThe documents are provided below:\n---\n${documentsText}\n---\n\nAnalyze the documents collectively and extract the following information. Structure your response as a single, valid JSON object according to the schema below.\n\n**JSON Schema:**\n{\n  \"keyEntities\": {\n    \"plaintiffs\": [\"string\"],\n    \"defendants\": [\"string\"],\n    \"judges\": [\"string\"],\n    \"other_parties\": [\"string\"]\n  },\n  \"keyFacts\": [\n    {\n      \"fact\": \"string\",\n      \"source_document\": \"string\"\n    }\n  ],\n  \"timelineOfEvents\": [\n    {\n      \"date\": \"YYYY-MM-DD\",\n      \"event\": \"string\",\n      \"source_document\": \"string\"\n    }\n  ],\n  \"legalContext\": {\n    \"statedClaims\": [\"string\"],\n    \"defenses\": [\"string\"]\n  }\n}\n\nReturn ONLY the valid JSON object.`\n  }]\n};\n\nreturn [{\n  json: {\n    ...inputData,\n    aiRequestPayload: aiRequestPayload,\n    processingMetadata: {\n      nodeType: \"ai-case-analysis-preparation\",\n      requestPreparedAt: new Date().toISOString(),\n      documentsTextLength: documentsText.length\n    }\n  },\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1504,
        288
      ],
      "id": "ai-prepare-case-docs-001-2025",
      "name": "Prepare AI Case Analysis Request"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.anthropic.com/v1/messages",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "httpHeaderAuth",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify($json.aiRequestPayload) }}",
        "options": {
          "timeout": 60000,
          "retry": {
            "enabled": true,
            "maxTries": 3,
            "waitBetween": 3000
          }
        },
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            },
            {
              "name": "anthropic-version",
              "value": "2023-06-01"
            }
          ]
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        1712,
        288
      ],
      "id": "ai-execute-case-docs-001-2025",
      "name": "Execute AI Case Analysis Request",
      "credentials": {
        "httpHeaderAuth": {
          "id": "anthropic-api-legal-workflow",
          "name": "Anthropic API - Legal Workflow"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Parse Case Analysis AI Response\n// This node processes the response from Anthropic Claude and validates the case analysis\n\nconst aiResponse = $input.first().json;\nconst originalData = $items(\"Prepare AI Case Analysis Request\")[0].json;\n\n// Initialize processing result\nconst processingResult = {\n  isValid: true,\n  errors: [],\n  analysisData: null,\n  processingTimestamp: new Date().toISOString(),\n  analysisMetadata: {\n    nodeType: \"ai-case-analysis-parser\",\n    processingBranch: \"case-documents-analysis\",\n    aiModel: \"claude-3-opus\"\n  }\n};\n\ntry {\n  // Check for API errors first\n  if (aiResponse.error) {\n    throw new Error(`API Error: ${aiResponse.error.message || 'Unknown API error'}`);\n  }\n  \n  // Extract AI response content\n  if (!aiResponse.content || !Array.isArray(aiResponse.content) || aiResponse.content.length === 0) {\n    throw new Error(\"Invalid API response: Missing content array\");\n  }\n  \n  const aiContent = aiResponse.content[0].text;\n  if (!aiContent) {\n    throw new Error(\"Invalid API response: Missing text content\");\n  }\n  \n  // Parse JSON response\n  let analysisData;\n  try {\n    analysisData = JSON.parse(aiContent);\n  } catch (parseError) {\n    throw new Error(`JSON parsing failed: ${parseError.message}`);\n  }\n  \n  // Validate required top-level fields\n  const requiredFields = ['keyEntities', 'keyFacts', 'timelineOfEvents', 'legalContext'];\n  for (const field of requiredFields) {\n    if (!analysisData[field]) {\n      throw new Error(`Missing required field: ${field}`);\n    }\n  }\n  \n  // Validate keyEntities structure\n  if (!analysisData.keyEntities.plaintiffs || !Array.isArray(analysisData.keyEntities.plaintiffs)) {\n    throw new Error(\"keyEntities.plaintiffs must be an array\");\n  }\n  \n  // Set successful analysis result\n  processingResult.analysisData = analysisData;\n  processingResult.analysisMetadata.entitiesExtracted = {\n    plaintiffs: analysisData.keyEntities.plaintiffs.length,\n    defendants: analysisData.keyEntities.defendants.length,\n    judges: analysisData.keyEntities.judges.length,\n    other_parties: analysisData.keyEntities.other_parties.length\n  };\n  processingResult.analysisMetadata.factsExtracted = analysisData.keyFacts.length;\n  processingResult.analysisMetadata.timelineEventsExtracted = analysisData.timelineOfEvents.length;\n  processingResult.analysisMetadata.tokenUsage = aiResponse.usage || {};\n  processingResult.message = \"Case documents successfully analyzed using live AI\";\n  \n  // Preserve original preparation data\n  processingResult.originalPreparationData = {\n    preparationResult: originalData,\n    routingData: originalData.originalRoutingData || {}\n  };\n  \n} catch (error) {\n  processingResult.isValid = false;\n  processingResult.errors.push(`AI analysis failed: ${error.message}`);\n  processingResult.analysisMetadata.rawResponse = aiResponse;\n  \n  // Implement fallback strategy for case documents\n  console.error('AI case analysis failed, using fallback', error.message);\n  \n  // Return simulated response structure as fallback\n  processingResult.analysisData = {\n    keyEntities: {\n      plaintiffs: [\"AI_PROCESSING_FAILED\"],\n      defendants: [\"REQUIRES_MANUAL_REVIEW\"],\n      judges: [\"UNKNOWN\"],\n      other_parties: [\"MANUAL_ANALYSIS_NEEDED\"]\n    },\n    keyFacts: [{\n      fact: \"AI processing failed - manual document review required\",\n      source_document: \"System Error\"\n    }],\n    timelineOfEvents: [{\n      date: new Date().toISOString().split('T')[0],\n      event: \"AI analysis failure - manual processing required\",\n      source_document: \"System Error\"\n    }],\n    legalContext: {\n      statedClaims: [\"Manual review required due to AI processing failure\"],\n      defenses: [\"Manual analysis needed\"]\n    }\n  };\n  \n  processingResult.isValid = true;\n  processingResult.fallbackUsed = true;\n  processingResult.message = \"Fallback case analysis used due to AI processing failure\";\n}\n\nreturn [{\n  json: processingResult,\n  binary: $items(\"Prepare AI Case Analysis Request\")[0].binary\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1920,
        288
      ],
      "id": "ai-parse-case-docs-001-2025",
      "name": "Parse AI Case Analysis Response"
    },
    {
      "parameters": {
        "jsCode": "// Parse & Structure Case Documents Results Node\n// This node receives the AI output from Parse AI Case Analysis Response,\n// validates the structure, and enriches it with processing metadata\n\n// Get the input data from the Parse AI Case Analysis Response node\nconst inputData = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Initialize the processing result\nconst processingResult = {\n  isValid: true,\n  errors: [],\n  enrichedData: null,\n  processingTimestamp: new Date().toISOString(),\n  processingMetadata: {\n    nodeType: \"parse-structure\",\n    processingBranch: \"case-documents-analysis\",\n    validationPassed: false,\n    enrichmentApplied: false\n  }\n};\n\n// Validate that we received valid analysis data\nif (!inputData || !inputData.isValid) {\n  processingResult.isValid = false;\n  processingResult.errors.push(\"Invalid input: Previous AI analysis step failed\");\n  if (inputData && inputData.errors) {\n    processingResult.errors = processingResult.errors.concat(inputData.errors);\n  }\n  return [{ json: processingResult, binary: binaryData }];\n}\n\n// Extract the JSON data from the analysis result\nlet analysisJsonData;\ntry {\n  if (inputData.analysisData) {\n    // Data is already parsed as an object from the analysis step\n    analysisJsonData = inputData.analysisData;\n  } else {\n    processingResult.isValid = false;\n    processingResult.errors.push(\"No analysis data found in input\");\n    return [{ json: processingResult, binary: binaryData }];\n  }\n} catch (error) {\n  processingResult.isValid = false;\n  processingResult.errors.push(`Failed to parse JSON: ${error.message}`);\n  return [{ json: processingResult, binary: binaryData }];\n}\n\n// Validate the presence of required top-level keys\nconst requiredKeys = ['keyEntities', 'keyFacts', 'timelineOfEvents', 'legalContext'];\nconst missingKeys = [];\n\nfor (const key of requiredKeys) {\n  if (!analysisJsonData[key] || (typeof analysisJsonData[key] !== 'object' && !Array.isArray(analysisJsonData[key]))) {\n    missingKeys.push(key);\n  }\n}\n\nif (missingKeys.length > 0) {\n  processingResult.isValid = false;\n  processingResult.errors.push(`Missing or invalid required keys: ${missingKeys.join(', ')}`);\n  return [{ json: processingResult, binary: binaryData }];\n}\n\n// Validation passed - proceed with enrichment\nprocessingResult.processingMetadata.validationPassed = true;\n\n// Create enriched data by adding processing metadata\nconst enrichedJsonData = {\n  ...analysisJsonData,\n  processing: {\n    analysisStatus: \"validated\",\n    parsedAt: new Date().toISOString(),\n    parserVersion: \"1.0.0\",\n    validatedKeys: requiredKeys,\n    originalAnalysisTimestamp: inputData.processingTimestamp,\n    analysisMetadata: inputData.analysisMetadata || {},\n    fallbackUsed: inputData.fallbackUsed || false\n  }\n};\n\n// Set successful processing result\nprocessingResult.enrichedData = enrichedJsonData;\nprocessingResult.processingMetadata.enrichmentApplied = true;\nprocessingResult.message = \"Case documents analysis successfully parsed, validated, and enriched\";\n\n// Preserve original analysis and preparation data for downstream processing\nprocessingResult.originalData = {\n  analysisResult: inputData,\n  preparationData: inputData.originalPreparationData || {}\n};\n\n// Return the enriched result\nreturn [{\n  json: processingResult,\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2128,
        288
      ],
      "id": "parse-structure-case-docs-001-2025",
      "name": "Parse & Structure Case Documents Results"
    },
    {
      "parameters": {
        "jsCode": "// Validate Case Documents Analysis Node\n// This node performs final quality check on the enriched case documents data\n// and adds final validation timestamp if processing status is validated\n\n// Get the input data from the Parse & Structure Case Documents Results node\nconst inputData = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Initialize the validation result\nconst validationResult = {\n  isValid: true,\n  errors: [],\n  validatedData: null,\n  processingTimestamp: new Date().toISOString(),\n  validationMetadata: {\n    nodeType: \"final-validation\",\n    processingBranch: \"case-documents-analysis\",\n    finalValidationApplied: false\n  }\n};\n\n// Validate that we received valid processing data\nif (!inputData || !inputData.isValid) {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"Invalid input: Previous processing step failed\");\n  if (inputData && inputData.errors) {\n    validationResult.errors = validationResult.errors.concat(inputData.errors);\n  }\n  return [{ json: validationResult, binary: binaryData }];\n}\n\n// Extract the enriched data\nconst enrichedData = inputData.enrichedData;\nif (!enrichedData) {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"No enriched data found in input\");\n  return [{ json: validationResult, binary: binaryData }];\n}\n\n// Validate Processing Status: Check if processing.analysisStatus === \"validated\"\nif (!enrichedData.processing || enrichedData.processing.analysisStatus !== \"validated\") {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"Final analysis validation failed: Analysis status is not 'validated'\");\n  validationResult.actualStatus = enrichedData.processing ? enrichedData.processing.analysisStatus : \"missing\";\n  return [{ json: validationResult, binary: binaryData }];\n}\n\n// Finalize Output: Add finalValidationTimestamp to the processing object\nconst finalValidatedData = {\n  ...enrichedData,\n  processing: {\n    ...enrichedData.processing,\n    finalValidationTimestamp: new Date().toISOString(),\n    finalValidationStatus: \"completed\"\n  }\n};\n\n// Set successful validation result\nvalidationResult.validatedData = finalValidatedData;\nvalidationResult.validationMetadata.finalValidationApplied = true;\nvalidationResult.message = \"Final validation completed successfully - case documents analysis validated\";\n\n// Preserve original processing and analysis data\nvalidationResult.originalData = {\n  processingResult: inputData,\n  analysisData: inputData.originalData || {}\n};\n\n// Return the final validated result\nreturn [{\n  json: validationResult,\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2232,
        288
      ],
      "id": "validate-case-docs-final-001-2025",
      "name": "Validate Case Documents Analysis"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          }
        },
        "combineOperation": "all",
        "options": {}
      },
      "type": "n8n-nodes-base.wait",
      "typeVersion": 1.1,
      "position": [
        2336,
        208
      ],
      "id": "wait-for-branches-001-2025",
      "name": "Wait for Both Branches"
    },
    {
      "parameters": {
        "jsCode": "// Case Data Merger Node\n// This node merges the validated outputs from both processing branches:\n// - Intake form extraction branch\n// - Case documents analysis branch\n// Creates a unified case file structure\n\n// Get inputs from both branches via the Wait node\nconst allInputs = $input.all();\n\n// Initialize merge result\nconst mergeResult = {\n  isValid: true,\n  errors: [],\n  unifiedCaseFile: null,\n  processingTimestamp: new Date().toISOString(),\n  mergeMetadata: {\n    nodeType: \"case-data-merger\",\n    branchesReceived: allInputs.length,\n    mergeStrategy: \"unified-case-file\",\n    mergeStatus: \"processing\"\n  }\n};\n\n// Validate that we received data from both branches\nif (!allInputs || allInputs.length === 0) {\n  mergeResult.isValid = false;\n  mergeResult.errors.push(\"No input data received from either branch\");\n  return [{ json: mergeResult }];\n}\n\n// Initialize branch data containers\nlet intakeBranchData = null;\nlet caseDocumentsBranchData = null;\n\n// Process each input to identify branch data\nfor (const input of allInputs) {\n  const inputJson = input.json;\n  \n  // Check if this is intake branch data\n  if (inputJson && inputJson.validationMetadata && \n      inputJson.validationMetadata.processingBranch === \"intake-analysis\") 