{
  "name": "Generate Findings Email",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "legal-analysis-upload",
        "responseMode": "responseNode",
        "options": {
          "allowedOrigins": "https://findingemail-0w07x.kinsta.page",
          "responseHeaders": {
            "entries": [
              {
                "name": "Access-Control-Allow-Origin",
                "value": "https://findingemail-0w07x.kinsta.page"
              },
              {
                "name": "Access-Control-Allow-Methods",
                "value": "POST, OPTIONS"
              },
              {
                "name": "Access-Control-Allow-Headers",
                "value": "Content-Type"
              }
            ]
          }
        }
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        -1776,
        -208
      ],
      "id": "82a89565-426d-4a6b-a3f2-ea702b39ea0b",
      "name": "Webhook",
      "webhookId": "6d7fe8eb-8786-4526-88ad-5cd8fbf1c6c6"
    },
    {
      "parameters": {
        "jsCode": "// Step 2: Data Structure Validation for Legal Document Analysis\n// This code validates incoming form data and ensures it matches our expected schema\n\n// First, let's extract the incoming data from the webhook\n// Based on Step 1 findings, form data is nested in the 'body' property\nconst incomingData = $input.first().json;\n\n// Initialize our validation results object\nconst validationResult = {\n  isValid: false,\n  errors: [],\n  validatedData: null,\n  processingTimestamp: new Date().toISOString()\n};\n\n// Check if the basic structure exists\n// We expect data to be in incomingData.body based on Step 1 findings\nif (!incomingData.body) {\n  validationResult.errors.push(\"Missing form data structure. Expected 'body' property not found.\");\n  return [{ json: validationResult }];\n}\n\nconst formData = incomingData.body;\n\n// Validate required fields according to Form_Output_Documentation.md\n// Required fields: clientName and attorneyName\nconst requiredFields = ['clientName', 'attorneyName'];\n\nfor (const field of requiredFields) {\n  if (!formData[field]) {\n    validationResult.errors.push(`Missing required field: ${field}`);\n  } else if (typeof formData[field] !== 'string') {\n    validationResult.errors.push(`Field ${field} must be a string`);\n  } else if (formData[field].trim().length === 0) {\n    validationResult.errors.push(`Field ${field} cannot be empty`);\n  }\n}\n\n// Validate optional field (caseReference) if present\nif (formData.caseReference && typeof formData.caseReference !== 'string') {\n  validationResult.errors.push(\"Field caseReference must be a string\");\n}\n\n// If we have errors, return them immediately\nif (validationResult.errors.length > 0) {\n  validationResult.isValid = false;\n  return [{ json: validationResult }];\n}\n\n// If validation passes, prepare clean data structure\nconst cleanData = {\n  clientName: formData.clientName.trim(),\n  caseReference: formData.caseReference ? formData.caseReference.trim() : '',\n  attorneyName: formData.attorneyName.trim()\n};\n\n// Set success status and return validated data\nvalidationResult.isValid = true;\nvalidationResult.validatedData = cleanData;\nvalidationResult.message = \"Form data validation successful\";\n\nreturn [{ json: validationResult }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1552,
        -208
      ],
      "id": "23ceec5e-7b3f-4ada-89e0-31043f460369",
      "name": "Validate Form Data"
    },
    {
      "parameters": {
        "jsCode": "// Step 3: Intake Form Data Processing\n// This node structures the validated intake form data into a standardized format\n// Based on the established pattern from the proposed workflow's caseInfo structure\n\nconst validationData = $input.first().json;\n\n// Only process if validation was successful\nif (!validationData.isValid) {\n  // Pass through validation errors unchanged\n  return [{ json: validationData }];\n}\n\n// Extract the clean validated data\nconst validatedFields = validationData.validatedData;\n\n// Structure the intake form data into standardized JSON format\n// Following the caseInfo pattern from the proposed workflow\nconst structuredIntakeData = {\n  caseInfo: {\n    clientName: validatedFields.clientName,\n    caseReference: validatedFields.caseReference || `CASE-${new Date().toISOString().split('T')[0]}`,\n    attorneyName: validatedFields.attorneyName,\n    processingDate: new Date().toISOString()\n  },\n  dataProcessingStatus: {\n    intakeFormProcessed: true,\n    structuredAt: new Date().toISOString(),\n    dataQuality: 'validated'\n  },\n  nextStage: 'document-processing'\n};\n\n// Return the structured data with success metadata\nreturn [{\n  json: {\n    isValid: true,\n    structuredData: structuredIntakeData,\n    processingTimestamp: validationData.processingTimestamp,\n    message: 'Intake form data successfully structured for processing'\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1328,
        -208
      ],
      "id": "462a4512-a812-4ef8-ae38-2b15a4e5f7c1",
      "name": "Structure Intake Data"
    },
    {
      "parameters": {
        "jsCode": "// Step 4: Binary Data Reception and File Cataloging\n// This node extracts binary files from webhook and creates file catalog\n\n// Get the structured intake data from previous step\nconst intakeData = $input.first().json;\n\n// Get the original webhook data that contains binary files\n// We need to look back at the webhook node's output\nconst webhookItems = $items(\"Webhook\");\nconst binaryData = webhookItems[0].binary || {};\n\n// Initialize file processing results\nconst fileProcessingResult = {\n  isValid: true,\n  errors: [],\n  filesCatalog: {\n    intakeForm: null,\n    caseDocuments: [],\n    totalFileSize: 0,\n    fileCount: 0\n  },\n  structuredData: intakeData.structuredData || {},\n  processingTimestamp: new Date().toISOString()\n};\n\n// Check if we have any binary data\nif (!binaryData || Object.keys(binaryData).length === 0) {\n  fileProcessingResult.isValid = false;\n  fileProcessingResult.errors.push(\"No files were uploaded with the form submission\");\n  return [{ json: fileProcessingResult }];\n}\n\n// Define allowed file types\nconst allowedMimeTypes = [\n  'application/pdf',\n  'application/vnd.openxmlformats-officedocument.wordprocessingml.document', // .docx\n  'application/msword', // .doc\n  'text/plain' // .txt\n];\n\n// Size limit: 100MB total\nconst MAX_TOTAL_SIZE = 100 * 1024 * 1024; // 100MB in bytes\nlet totalSize = 0;\n\n// Process intake form (required)\nif (binaryData.intakeForm) {\n  const intakeFile = binaryData.intakeForm;\n  \n  // Validate file type\n  if (!allowedMimeTypes.includes(intakeFile.mimeType)) {\n    fileProcessingResult.errors.push(`Intake form has invalid file type: ${intakeFile.mimeType}`);\n  }\n  \n  // Extract metadata with proper size handling\n  const sizeInBytes = typeof intakeFile.fileSize === 'number'\n    ? intakeFile.fileSize\n    : intakeFile.data ? intakeFile.data.length : 0;\n  \n  fileProcessingResult.filesCatalog.intakeForm = {\n    fileName: intakeFile.fileName,\n    fileSize: sizeInBytes,\n    mimeType: intakeFile.mimeType,\n    uploadedAt: new Date().toISOString()\n  };\n  \n  totalSize += sizeInBytes;\n  fileProcessingResult.filesCatalog.fileCount++;\n} else {\n  fileProcessingResult.isValid = false;\n  fileProcessingResult.errors.push(\"Required intake form is missing\");\n}\n\n// Process case documents\nlet documentIndex = 0;\nlet hasAtLeastOneDocument = false;\n\nwhile (binaryData[`caseDocument${documentIndex}`]) {\n  const docFile = binaryData[`caseDocument${documentIndex}`];\n  hasAtLeastOneDocument = true;\n  \n  // Validate file type\n  if (!allowedMimeTypes.includes(docFile.mimeType)) {\n    fileProcessingResult.errors.push(`Case document ${documentIndex} has invalid file type: ${docFile.mimeType}`);\n  }\n  \n  // Extract metadata with proper size handling\n  const sizeInBytes = typeof docFile.fileSize === 'number'\n    ? docFile.fileSize\n    : docFile.data ? docFile.data.length : 0;\n  \n  // Add to catalog\n  fileProcessingResult.filesCatalog.caseDocuments.push({\n    index: documentIndex,\n    fileName: docFile.fileName,\n    fileSize: sizeInBytes,\n    mimeType: docFile.mimeType,\n    uploadedAt: new Date().toISOString()\n  });\n  \n  totalSize += sizeInBytes;\n  fileProcessingResult.filesCatalog.fileCount++;\n  documentIndex++;\n}\n\n// Validate at least one case document\nif (!hasAtLeastOneDocument) {\n  fileProcessingResult.isValid = false;\n  fileProcessingResult.errors.push(\"At least one case document is required\");\n}\n\n// Check total size limit\nfileProcessingResult.filesCatalog.totalFileSize = totalSize;\nif (totalSize > MAX_TOTAL_SIZE) {\n  fileProcessingResult.isValid = false;\n  fileProcessingResult.errors.push(`Total file size (${(totalSize / 1024 / 1024).toFixed(2)}MB) exceeds 100MB limit`);\n}\n\n// If there are validation errors, mark as invalid\nif (fileProcessingResult.errors.length > 0) {\n  fileProcessingResult.isValid = false;\n}\n\n// Merge with intake data if valid\nif (fileProcessingResult.isValid && intakeData.isValid) {\n  fileProcessingResult.message = \"Files successfully received and cataloged\";\n  fileProcessingResult.structuredData.filesCatalog = fileProcessingResult.filesCatalog;\n  fileProcessingResult.structuredData.dataProcessingStatus.filesProcessed = true;\n  fileProcessingResult.structuredData.dataProcessingStatus.filesProcessedAt = new Date().toISOString();\n  fileProcessingResult.structuredData.nextStage = 'document-categorization';\n}\n\n// Pass binary data forward for downstream processing\nreturn [{\n  json: fileProcessingResult,\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1104,
        -208
      ],
      "id": "7b42c597-966d-4190-ba66-4d1e5ecf2e57",
      "name": "Extract Binary Files"
    },
    {
      "parameters": {
        "jsCode": "// Step 5: Document Categorization Router\n// This node creates separate processing branches for intake forms and case documents\n// Based on the file catalog from Step 4, it prepares data for parallel processing\n\n// Get the file processing result from Step 4\nconst step4Data = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Initialize the routing results\nconst routingResults = {\n  isValid: true,\n  errors: [],\n  branches: [],\n  processingTimestamp: new Date().toISOString()\n};\n\n// Validate that we have the necessary data from Step 4\nif (!step4Data || !step4Data.isValid) {\n  routingResults.isValid = false;\n  routingResults.errors.push(\"Invalid data received from Step 4 - file processing failed\");\n  return [{ json: routingResults }];\n}\n\n// Extract the structured data and file catalog\nconst structuredData = step4Data.structuredData || {};\nconst filesCatalog = structuredData.filesCatalog || step4Data.filesCatalog;\n\nif (!filesCatalog) {\n  routingResults.isValid = false;\n  routingResults.errors.push(\"File catalog is missing from Step 4 data\");\n  return [{ json: routingResults }];\n}\n\n// Prepare output items for the Switch node\nconst outputItems = [];\n\n// Branch 1: Intake Form Processing\nif (filesCatalog.intakeForm) {\n  const intakeFormBranch = {\n    documentType: \"intakeForm\",\n    caseInfo: structuredData.caseInfo || {},\n    intakeFormData: {\n      fileName: filesCatalog.intakeForm.fileName,\n      fileSize: filesCatalog.intakeForm.fileSize,\n      mimeType: filesCatalog.intakeForm.mimeType,\n      uploadedAt: filesCatalog.intakeForm.uploadedAt,\n      binaryDataReference: \"intakeForm\"\n    },\n    processingMetadata: {\n      branch: \"intake-analysis\",\n      documentCount: 1,\n      processingStartedAt: new Date().toISOString(),\n      priority: \"high\"\n    },\n    originalData: {\n      step4Result: step4Data,\n      totalFileCount: filesCatalog.fileCount,\n      totalFileSize: filesCatalog.totalFileSize\n    }\n  };\n  \n  outputItems.push({\n    json: intakeFormBranch,\n    binary: { intakeForm: binaryData.intakeForm }\n  });\n  \n  routingResults.branches.push(\"intake-analysis\");\n} else {\n  routingResults.isValid = false;\n  routingResults.errors.push(\"Intake form is required but missing from file catalog\");\n}\n\n// Branch 2: Case Documents Processing\nif (filesCatalog.caseDocuments && filesCatalog.caseDocuments.length > 0) {\n  // Prepare binary data object for case documents\n  const caseDocumentsBinary = {};\n  \n  // Add each case document to binary data\n  filesCatalog.caseDocuments.forEach((doc, index) => {\n    const binaryKey = `caseDocument${doc.index}`;\n    if (binaryData[binaryKey]) {\n      caseDocumentsBinary[binaryKey] = binaryData[binaryKey];\n    }\n  });\n  \n  const caseDocumentsBranch = {\n    documentType: \"caseDocuments\",\n    caseInfo: structuredData.caseInfo || {},\n    caseDocumentsData: {\n      documentCount: filesCatalog.caseDocuments.length,\n      totalSize: filesCatalog.caseDocuments.reduce((total, doc) => total + doc.fileSize, 0),\n      documents: filesCatalog.caseDocuments.map(doc => ({\n        index: doc.index,\n        fileName: doc.fileName,\n        fileSize: doc.fileSize,\n        mimeType: doc.mimeType,\n        uploadedAt: doc.uploadedAt,\n        binaryDataReference: `caseDocument${doc.index}`\n      }))\n    },\n    processingMetadata: {\n      branch: \"case-documents-analysis\",\n      documentCount: filesCatalog.caseDocuments.length,\n      processingStartedAt: new Date().toISOString(),\n      priority: \"normal\"\n    },\n    originalData: {\n      step4Result: step4Data,\n      totalFileCount: filesCatalog.fileCount,\n      totalFileSize: filesCatalog.totalFileSize\n    }\n  };\n  \n  outputItems.push({\n    json: caseDocumentsBranch,\n    binary: caseDocumentsBinary\n  });\n  \n  routingResults.branches.push(\"case-documents-analysis\");\n} else {\n  routingResults.isValid = false;\n  routingResults.errors.push(\"At least one case document is required but none found in file catalog\");\n}\n\n// If we have validation errors, return error response\nif (routingResults.errors.length > 0) {\n  routingResults.isValid = false;\n  return [{ json: routingResults }];\n}\n\n// Return all output items for the Switch node\nreturn outputItems;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -880,
        -208
      ],
      "id": "47614ad3-f9d9-4286-ad60-25a5eba9c887",
      "name": "Categorize Documents"
    },
    {
      "parameters": {
        "conditions": {
          "string": [
            {
              "value1": "={{ $json.documentType }}",
              "value2": "intakeForm"
            }
          ]
        }
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        -208,
        -304
      ],
      "id": "e83366cd-4c7e-435d-924f-28fea536d57b",
      "name": "Route Intake Form"
    },
    {
      "parameters": {
        "conditions": {
          "string": [
            {
              "value1": "={{ $json.documentType }}",
              "value2": "caseDocuments"
            }
          ]
        }
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        -656,
        -112
      ],
      "id": "7380300f-f26f-40e7-bc4f-28aecee7dc55",
      "name": "Route Case Documents"
    },
    {
      "parameters": {
        "jsCode": "// Parse Intake Form Extraction Response\n// Validates and structures the OpenAI response for intake form data\n\nconst openAIInput = $input.first().json;\n// Remove the problematic line that references a non-existent node\n// const promptData = $items(\"Prepare OpenAI Prompt\")[0].json;\n\n// Get routing data from the OpenAI node input instead\nconst routingItems = $items(\"Route Intake Form\");\nconst routingData = routingItems && routingItems[0] ? routingItems[0].json : {};\n\n// Initialize extraction result\nconst extractionResult = {\n  isValid: true,\n  errors: [],\n  extractedData: null,\n  processingTimestamp: new Date().toISOString(),\n  extractionMetadata: {\n    nodeType: \"parse-openai-extraction\",\n    processingBranch: \"intake-analysis\",\n    aiModel: \"gpt-4o-mini\",\n    extractionProcessed: false\n  }\n};\n\n// Extract OpenAI response\nlet responseText;\nif (openAIInput.message && openAIInput.message.content) {\n  responseText = openAIInput.message.content.trim();\n} else {\n  extractionResult.isValid = false;\n  extractionResult.errors.push(\"No message content found in OpenAI response\");\n  return [{ json: extractionResult }];\n}\n\n// Clean and parse JSON response\ntry {\n  // Remove any markdown formatting\n  const cleanText = responseText.replace(/```json|```/g, '').trim();\n  \n  // Validate JSON structure\n  if (!cleanText.startsWith('{') || !cleanText.endsWith('}')) {\n    throw new Error('Response is not properly formatted JSON');\n  }\n  \n  const parsedData = JSON.parse(cleanText);\n  \n  // Validate required schema fields\n  const requiredFields = ['clientInfo', 'caseInfo', 'attorneyInfo'];\n  const missingFields = [];\n  \n  for (const field of requiredFields) {\n    if (!parsedData[field] || typeof parsedData[field] !== 'object') {\n      missingFields.push(field);\n    }\n  }\n  \n  if (missingFields.length > 0) {\n    throw new Error(`Missing or invalid required fields: ${missingFields.join(', ')}`);\n  }\n  \n  // Validate specific required subfields\n  if (!parsedData.clientInfo.clientName) {\n    throw new Error('Missing required field: clientInfo.clientName');\n  }\n  if (!parsedData.caseInfo.caseReference) {\n    throw new Error('Missing required field: caseInfo.caseReference');\n  }\n  if (!parsedData.attorneyInfo.attorneyName) {\n    throw new Error('Missing required field: attorneyInfo.attorneyName');\n  }\n  \n  // Set successful extraction result\n  extractionResult.extractedData = parsedData;\n  extractionResult.extractionMetadata.extractionProcessed = true;\n  extractionResult.extractionMetadata.fieldsExtracted = [\n    \"clientInfo.clientName\",\n    \"clientInfo.contactInfo\",\n    \"caseInfo.caseReference\", \n    \"caseInfo.summary\",\n    \"attorneyInfo.attorneyName\"\n  ];\n  extractionResult.message = \"Intake form data successfully extracted using OpenAI\";\n  \n} catch (error) {\n  extractionResult.isValid = false;\n  extractionResult.errors.push(`JSON parsing failed: ${error.message}`);\n  extractionResult.rawResponse = responseText;\n  \n  // Log for debugging\n  console.error('OpenAI Extraction Response Parsing Error:', {\n    error: error.message,\n    rawResponse: responseText.substring(0, 200) + '...'\n  });\n}\n\n// Preserve original routing data for downstream processing using available data\nextractionResult.originalRoutingData = {\n  documentType: routingData.documentType || \"intakeForm\",\n  caseInfo: routingData.caseInfo || {},\n  intakeFormData: routingData.intakeFormData || {},\n  processingMetadata: routingData.processingMetadata || {}\n};\n\n// Return the extraction result\nreturn [{ json: extractionResult }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        416,
        -304
      ],
      "id": "44033c4c-8fed-43f7-8fc4-861f0ded537f",
      "name": "Parse Intake Extraction Response"
    },
    {
      "parameters": {
        "jsCode": "// Parse & Enrich Intake Data Node\n// This node receives the extraction output and enriches it with processing metadata\n\n// Get the input data from the Parse Intake Extraction Response node\nconst inputData = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Initialize the processing result\nconst processingResult = {\n  isValid: true,\n  errors: [],\n  enrichedData: null,\n  processingTimestamp: new Date().toISOString(),\n  processingMetadata: {\n    nodeType: \"parse-enrich\",\n    processingBranch: \"intake-analysis\",\n    validationPassed: false,\n    enrichmentApplied: false\n  }\n};\n\n// Validate that we received valid extraction data\nif (!inputData || !inputData.isValid) {\n  processingResult.isValid = false;\n  processingResult.errors.push(\"Invalid input: Previous extraction step failed\");\n  if (inputData && inputData.errors) {\n    processingResult.errors = processingResult.errors.concat(inputData.errors);\n  }\n  return [{ json: processingResult, binary: binaryData }];\n}\n\n// Extract the JSON data from the extraction result\nlet extractedJsonData;\ntry {\n  if (inputData.extractedData) {\n    // Data is already parsed as an object from the extraction step\n    extractedJsonData = inputData.extractedData;\n  } else {\n    processingResult.isValid = false;\n    processingResult.errors.push(\"No extracted data found in input\");\n    return [{ json: processingResult, binary: binaryData }];\n  }\n} catch (error) {\n  processingResult.isValid = false;\n  processingResult.errors.push(`Failed to parse JSON: ${error.message}`);\n  return [{ json: processingResult, binary: binaryData }];\n}\n\n// Validate the presence of required top-level keys\nconst requiredKeys = ['clientInfo', 'caseInfo', 'attorneyInfo'];\nconst missingKeys = [];\n\nfor (const key of requiredKeys) {\n  if (!extractedJsonData[key] || typeof extractedJsonData[key] !== 'object') {\n    missingKeys.push(key);\n  }\n}\n\nif (missingKeys.length > 0) {\n  processingResult.isValid = false;\n  processingResult.errors.push(`Missing or invalid required keys: ${missingKeys.join(', ')}`);\n  return [{ json: processingResult, binary: binaryData }];\n}\n\n// Validation passed - proceed with enrichment\nprocessingResult.processingMetadata.validationPassed = true;\n\n// Create enriched data by adding processing metadata\nconst enrichedJsonData = {\n  ...extractedJsonData,\n  processing: {\n    extractionStatus: \"validated\",\n    parsedAt: new Date().toISOString(),\n    parserVersion: \"1.0.0\",\n    validatedKeys: requiredKeys,\n    originalExtractionTimestamp: inputData.processingTimestamp,\n    extractionMetadata: inputData.extractionMetadata || {}\n  }\n};\n\n// Set successful processing result\nprocessingResult.enrichedData = enrichedJsonData;\nprocessingResult.processingMetadata.enrichmentApplied = true;\nprocessingResult.message = \"Intake data successfully parsed, validated, and enriched\";\n\n// Preserve original routing and extraction data for downstream processing\nprocessingResult.originalData = {\n  extractionResult: inputData,\n  routingData: inputData.originalRoutingData || {}\n};\n\n// Return the enriched result\nreturn [{\n  json: processingResult,\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        640,
        -304
      ],
      "id": "674cb383-2e88-466b-95cb-20c9c82a799e",
      "name": "Parse & Enrich Intake Data"
    },
    {
      "parameters": {
        "jsCode": "// Validate Intake Extraction Node\n// This node performs final quality check on the enriched intake data\n// and adds final validation timestamp if processing status is validated\n\n// Get the input data from the Parse & Enrich Intake Data node\nconst inputData = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Initialize the validation result\nconst validationResult = {\n  isValid: true,\n  errors: [],\n  validatedData: null,\n  processingTimestamp: new Date().toISOString(),\n  validationMetadata: {\n    nodeType: \"final-validation\",\n    processingBranch: \"intake-analysis\",\n    finalValidationApplied: false\n  }\n};\n\n// Validate that we received valid processing data\nif (!inputData || !inputData.isValid) {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"Invalid input: Previous processing step failed\");\n  if (inputData && inputData.errors) {\n    validationResult.errors = validationResult.errors.concat(inputData.errors);\n  }\n  return [{ json: validationResult, binary: binaryData }];\n}\n\n// Extract the enriched data\nconst enrichedData = inputData.enrichedData;\nif (!enrichedData) {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"No enriched data found in input\");\n  return [{ json: validationResult, binary: binaryData }];\n}\n\n// Validate Processing Status: Check if processing.extractionStatus === \"validated\"\nif (!enrichedData.processing || enrichedData.processing.extractionStatus !== \"validated\") {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"Final validation failed: Extraction status is not 'validated'\");\n  validationResult.actualStatus = enrichedData.processing ? enrichedData.processing.extractionStatus : \"missing\";\n  return [{ json: validationResult, binary: binaryData }];\n}\n\n// Finalize Output: Add finalValidationTimestamp to the processing object\nconst finalValidatedData = {\n  ...enrichedData,\n  processing: {\n    ...enrichedData.processing,\n    finalValidationTimestamp: new Date().toISOString(),\n    finalValidationStatus: \"completed\"\n  }\n};\n\n// Set successful validation result\nvalidationResult.validatedData = finalValidatedData;\nvalidationResult.validationMetadata.finalValidationApplied = true;\nvalidationResult.message = \"Final validation completed successfully - intake extraction validated\";\n\n// Preserve original processing and routing data\nvalidationResult.originalData = {\n  processingResult: inputData,\n  extractionData: inputData.originalData || {}\n};\n\n// Return the final validated result\nreturn [{\n  json: validationResult,\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        864,
        -304
      ],
      "id": "6d73cf36-84cf-4a50-8f4b-d045d860268b",
      "name": "Validate Intake Extraction"
    },
    {
      "parameters": {
        "jsCode": "// Prepare Case Documents Node\n// This node receives case documents data and simulates text extraction\n// from multiple files, aggregating them into a single text string\n\n// Get the input data from the Route Case Documents node\nconst inputData = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Initialize preparation results\nconst preparationResult = {\n  isValid: true,\n  errors: [],\n  aggregatedDocumentsText: null,\n  processingTimestamp: new Date().toISOString(),\n  preparationMetadata: {\n    nodeType: \"prepare-documents\",\n    processingBranch: \"case-documents-analysis\",\n    simulatedProcessing: true\n  }\n};\n\n// Validate input data structure\nif (!inputData || inputData.documentType !== \"caseDocuments\") {\n  preparationResult.isValid = false;\n  preparationResult.errors.push(\"Invalid input: Expected caseDocuments document type\");\n  return [{ json: preparationResult }];\n}\n\n// Check for case documents data\nif (!inputData.caseDocumentsData || !inputData.caseDocumentsData.documents) {\n  preparationResult.isValid = false;\n  preparationResult.errors.push(\"Missing caseDocumentsData.documents array\");\n  return [{ json: preparationResult }];\n}\n\nconst documents = inputData.caseDocumentsData.documents;\nif (!Array.isArray(documents) || documents.length === 0) {\n  preparationResult.isValid = false;\n  preparationResult.errors.push(\"caseDocumentsData.documents must be a non-empty array\");\n  return [{ json: preparationResult }];\n}\n\n// SIMULATION: Text Extraction from Multiple Documents\n// Since we cannot yet read text from multiple files, we simulate this step\nconst aggregatedTextParts = [];\nconst documentSeparator = \"\\n\\n--- DOCUMENT SEPARATOR ---\\n\\n\";\n\n// Iterate through each document and create simulated text content\ndocuments.forEach((doc, index) => {\n  // Create distinct placeholder text for each document\n  const documentNumber = index + 1;\n  const simulatedText = `DOCUMENT ${documentNumber}: ${doc.fileName || `Document_${documentNumber}`}\n\nFile Information:\n- File Name: ${doc.fileName || `Document_${documentNumber}.pdf`}\n- File Size: ${doc.fileSize ? (doc.fileSize / 1024).toFixed(2) + ' KB' : 'Unknown'}\n- MIME Type: ${doc.mimeType || 'application/pdf'}\n- Upload Time: ${doc.uploadedAt || new Date().toISOString()}\n\nSimulated Document Content:\nThis is simulated content for document ${documentNumber}. In a production environment, this would contain the actual extracted text from the ${doc.mimeType || 'PDF'} file. The document appears to be a legal document related to the case proceedings.\n\nKey sections that might be found in this document:\n- Case details and background information\n- Legal arguments and supporting evidence\n- Timeline of events relevant to the case\n- Party information and contact details\n- Court filings and procedural documents\n- Evidence and supporting documentation\n\nThis simulated text represents what would typically be extracted from document ${documentNumber} in the case file. The actual content would provide detailed legal information relevant to the case analysis and would be used for downstream AI processing and analysis.\n\nEnd of simulated content for document ${documentNumber}.`;\n  \n  aggregatedTextParts.push(simulatedText);\n});\n\n// Combine all document texts with separators\nconst aggregatedText = aggregatedTextParts.join(documentSeparator);\n\n// Add final metadata footer\nconst metadataFooter = `\\n\\n--- END OF DOCUMENTS ---\\n\\nDocument Processing Summary:\n- Total Documents Processed: ${documents.length}\n- Total Aggregated Text Length: ${aggregatedText.length} characters\n- Processing Method: Simulated text extraction\n- Processing Timestamp: ${new Date().toISOString()}\n- Branch: case-documents-analysis`;\n\nconst finalAggregatedText = aggregatedText + metadataFooter;\n\n// Set successful preparation result\npreparationResult.aggregatedDocumentsText = finalAggregatedText;\npreparationResult.preparationMetadata.documentsProcessed = documents.length;\npreparationResult.preparationMetadata.totalTextLength = finalAggregatedText.length;\npreparationResult.preparationMetadata.processingMethod = \"simulated-extraction\";\npreparationResult.message = `Successfully prepared ${documents.length} case documents for analysis`;\n\n// Preserve original routing data for downstream processing\npreparationResult.originalRoutingData = {\n  documentType: inputData.documentType,\n  caseInfo: inputData.caseInfo,\n  caseDocumentsData: inputData.caseDocumentsData,\n  processingMetadata: inputData.processingMetadata\n};\n\n// Return the preparation result with aggregated text\nreturn [{\n  json: preparationResult,\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -432,
        -112
      ],
      "id": "bc5c9128-292c-4cec-8df2-50cff194bf05",
      "name": "Prepare Case Documents"
    },
    {
      "parameters": {
        "jsCode": "// Prepare Case Documents Analysis Prompt\n// Takes aggregated text from Prepare Case Documents and creates OpenAI prompt\n\nconst inputData = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Validate input from Prepare Case Documents node\nif (!inputData || !inputData.isValid || !inputData.aggregatedDocumentsText) {\n  return [{\n    json: {\n      isValid: false,\n      errors: [\"Invalid input: Missing aggregated documents text from preparation step\"],\n      processingTimestamp: new Date().toISOString()\n    }\n  }];\n}\n\nconst documentsText = inputData.aggregatedDocumentsText;\n\n// Create the analysis prompt using your existing template\nconst analysisPrompt = `You are a specialized legal analysis AI. You will be given a collection of case documents. Your task is to perform a comprehensive analysis across all provided documents and extract key legal information.\n\nThe documents are provided below:\n---\n${documentsText}\n---\n\nAnalyze the documents collectively and extract the following information. Structure your response as a single, valid JSON object according to the schema below.\n\n**JSON Schema:**\n{\n  \"keyEntities\": {\n    \"plaintiffs\": [\"string\"],\n    \"defendants\": [\"string\"],\n    \"judges\": [\"string\"],\n    \"other_parties\": [\"string\"]\n  },\n  \"keyFacts\": [\n    {\n      \"fact\": \"string\",\n      \"source_document\": \"string\"\n    }\n  ],\n  \"timelineOfEvents\": [\n    {\n      \"date\": \"YYYY-MM-DD\",\n      \"event\": \"string\",\n      \"source_document\": \"string\"\n    }\n  ],\n  \"legalContext\": {\n    \"statedClaims\": [\"string\"],\n    \"defenses\": [\"string\"]\n  }\n}\n\nReturn ONLY the valid JSON object.`;\n\n// Return the prompt with preserved metadata\nreturn [{\n  json: {\n    openaiPrompt: analysisPrompt,\n    originalData: inputData,\n    processingTimestamp: new Date().toISOString(),\n    promptMetadata: {\n      documentsTextLength: documentsText.length,\n      preparationData: inputData.preparationMetadata || {},\n      promptType: \"case-documents-analysis\"\n    }\n  },\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -208,
        -112
      ],
      "id": "ea71588b-8687-45e3-baee-940f4b08d879",
      "name": "Prepare Case Docs Analysis Prompt"
    },
    {
      "parameters": {
        "jsCode": "// Parse Case Documents Analysis Response\n// Validates and structures the OpenAI response\n\nconst openAIInput = $input.first().json;\nconst originalData = openAIInput.originalData || {};\n\n// Initialize processing result\nconst processingResult = {\n  isValid: true,\n  errors: [],\n  analysisData: null,\n  processingTimestamp: new Date().toISOString(),\n  analysisMetadata: {\n    nodeType: \"parse-openai-response\",\n    processingBranch: \"case-documents-analysis\",\n    aiModel: \"gpt-4o\",\n    responseProcessed: false\n  }\n};\n\n// Extract OpenAI response\nlet responseText;\nif (openAIInput.message && openAIInput.message.content) {\n  responseText = openAIInput.message.content.trim();\n} else {\n  processingResult.isValid = false;\n  processingResult.errors.push(\"No message content found in OpenAI response\");\n  return [{ json: processingResult }];\n}\n\n// Clean and parse JSON response\ntry {\n  // Remove any markdown formatting\n  const cleanText = responseText.replace(/```json|```/g, '').trim();\n  \n  // Validate JSON structure\n  if (!cleanText.startsWith('{') || !cleanText.endsWith('}')) {\n    throw new Error('Response is not properly formatted JSON');\n  }\n  \n  const parsedData = JSON.parse(cleanText);\n  \n  // Validate required schema fields\n  const requiredFields = ['keyEntities', 'keyFacts', 'timelineOfEvents', 'legalContext'];\n  const missingFields = [];\n  \n  for (const field of requiredFields) {\n    if (!parsedData[field]) {\n      missingFields.push(field);\n    }\n  }\n  \n  if (missingFields.length > 0) {\n    throw new Error(`Missing required fields: ${missingFields.join(', ')}`);\n  }\n  \n  // Validate keyEntities structure\n  if (!parsedData.keyEntities.plaintiffs || !Array.isArray(parsedData.keyEntities.plaintiffs)) {\n    throw new Error('keyEntities.plaintiffs must be an array');\n  }\n  \n  // Set successful result\n  processingResult.analysisData = parsedData;\n  processingResult.analysisMetadata.responseProcessed = true;\n  processingResult.analysisMetadata.entitiesFound = {\n    plaintiffs: parsedData.keyEntities.plaintiffs.length,\n    defendants: parsedData.keyEntities.defendants.length,\n    judges: parsedData.keyEntities.judges.length,\n    other_parties: parsedData.keyEntities.other_parties.length\n  };\n  processingResult.analysisMetadata.factsExtracted = parsedData.keyFacts.length;\n  processingResult.analysisMetadata.timelineEvents = parsedData.timelineOfEvents.length;\n  processingResult.analysisMetadata.claimsFound = parsedData.legalContext.statedClaims.length;\n  processingResult.analysisMetadata.defensesFound = parsedData.legalContext.defenses.length;\n  processingResult.message = \"Case documents analysis completed successfully with OpenAI\";\n  \n} catch (error) {\n  processingResult.isValid = false;\n  processingResult.errors.push(`JSON parsing failed: ${error.message}`);\n  processingResult.rawResponse = responseText;\n  \n  // Log for debugging\n  console.error('OpenAI Response Parsing Error:', {\n    error: error.message,\n    rawResponse: responseText.substring(0, 200) + '...'\n  });\n}\n\n// Preserve original data for downstream processing\nprocessingResult.originalPreparationData = {\n  preparationResult: originalData,\n  routingData: originalData.originalRoutingData || {}\n};\n\n// Return the processing result\nreturn [{ json: processingResult }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        416,
        -112
      ],
      "id": "3c1bb54f-4657-494e-a6d9-349c1ea7e6cb",
      "name": "Parse Case Docs Analysis Response"
    },
    {
      "parameters": {
        "jsCode": "// Parse & Structure Case Documents Results Node\n// This node receives the OpenAI output and enriches it with processing metadata\n\n// Get the input data from the Parse Case Docs Analysis Response node\nconst inputData = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Initialize the processing result\nconst processingResult = {\n  isValid: true,\n  errors: [],\n  enrichedData: null,\n  processingTimestamp: new Date().toISOString(),\n  processingMetadata: {\n    nodeType: \"parse-structure\",\n    processingBranch: \"case-documents-analysis\",\n    validationPassed: false,\n    enrichmentApplied: false\n  }\n};\n\n// Validate that we received valid analysis data\nif (!inputData || !inputData.isValid) {\n  processingResult.isValid = false;\n  processingResult.errors.push(\"Invalid input: Previous analysis step failed\");\n  if (inputData && inputData.errors) {\n    processingResult.errors = processingResult.errors.concat(inputData.errors);\n  }\n  return [{ json: processingResult, binary: binaryData }];\n}\n\n// Extract the JSON data from the analysis result\nlet analysisJsonData;\ntry {\n  if (inputData.analysisData) {\n    // Data is already parsed as an object from the analysis step\n    analysisJsonData = inputData.analysisData;\n  } else {\n    processingResult.isValid = false;\n    processingResult.errors.push(\"No analysis data found in input\");\n    return [{ json: processingResult, binary: binaryData }];\n  }\n} catch (error) {\n  processingResult.isValid = false;\n  processingResult.errors.push(`Failed to parse JSON: ${error.message}`);\n  return [{ json: processingResult, binary: binaryData }];\n}\n\n// Validate the presence of required top-level keys\nconst requiredKeys = ['keyEntities', 'keyFacts', 'timelineOfEvents', 'legalContext'];\nconst missingKeys = [];\n\nfor (const key of requiredKeys) {\n  if (!analysisJsonData[key] || (typeof analysisJsonData[key] !== 'object' && !Array.isArray(analysisJsonData[key]))) {\n    missingKeys.push(key);\n  }\n}\n\nif (missingKeys.length > 0) {\n  processingResult.isValid = false;\n  processingResult.errors.push(`Missing or invalid required keys: ${missingKeys.join(', ')}`);\n  return [{ json: processingResult, binary: binaryData }];\n}\n\n// Validation passed - proceed with enrichment\nprocessingResult.processingMetadata.validationPassed = true;\n\n// Create enriched data by adding processing metadata\nconst enrichedJsonData = {\n  ...analysisJsonData,\n  processing: {\n    analysisStatus: \"validated\",\n    parsedAt: new Date().toISOString(),\n    parserVersion: \"1.0.0\",\n    validatedKeys: requiredKeys,\n    originalAnalysisTimestamp: inputData.processingTimestamp,\n    analysisMetadata: inputData.analysisMetadata || {}\n  }\n};\n\n// Set successful processing result\nprocessingResult.enrichedData = enrichedJsonData;\nprocessingResult.processingMetadata.enrichmentApplied = true;\nprocessingResult.message = \"Case documents analysis successfully parsed, validated, and enriched\";\n\n// Preserve original analysis and preparation data for downstream processing\nprocessingResult.originalData = {\n  analysisResult: inputData,\n  preparationData: inputData.originalPreparationData || {}\n};\n\n// Return the enriched result\nreturn [{\n  json: processingResult,\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        640,
        -112
      ],
      "id": "14822ca9-b452-447b-9ad4-15c645bb036b",
      "name": "Parse & Structure Case Documents Results"
    },
    {
      "parameters": {
        "jsCode": "// Validate Case Documents Analysis Node\n// This node performs final quality check on the enriched case documents data\n// and adds final validation timestamp if processing status is validated\n\n// Get the input data from the Parse & Structure Case Documents Results node\nconst inputData = $input.first().json;\nconst binaryData = $input.first().binary || {};\n\n// Initialize the validation result\nconst validationResult = {\n  isValid: true,\n  errors: [],\n  validatedData: null,\n  processingTimestamp: new Date().toISOString(),\n  validationMetadata: {\n    nodeType: \"final-validation\",\n    processingBranch: \"case-documents-analysis\",\n    finalValidationApplied: false\n  }\n};\n\n// Validate that we received valid processing data\nif (!inputData || !inputData.isValid) {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"Invalid input: Previous processing step failed\");\n  if (inputData && inputData.errors) {\n    validationResult.errors = validationResult.errors.concat(inputData.errors);\n  }\n  return [{ json: validationResult, binary: binaryData }];\n}\n\n// Extract the enriched data\nconst enrichedData = inputData.enrichedData;\nif (!enrichedData) {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"No enriched data found in input\");\n  return [{ json: validationResult, binary: binaryData }];\n}\n\n// Validate Processing Status: Check if processing.analysisStatus === \"validated\"\nif (!enrichedData.processing || enrichedData.processing.analysisStatus !== \"validated\") {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"Final analysis validation failed: Analysis status is not 'validated'\");\n  validationResult.actualStatus = enrichedData.processing ? enrichedData.processing.analysisStatus : \"missing\";\n  return [{ json: validationResult, binary: binaryData }];\n}\n\n// Finalize Output: Add finalValidationTimestamp to the processing object\nconst finalValidatedData = {\n  ...enrichedData,\n  processing: {\n    ...enrichedData.processing,\n    finalValidationTimestamp: new Date().toISOString(),\n    finalValidationStatus: \"completed\"\n  }\n};\n\n// Set successful validation result\nvalidationResult.validatedData = finalValidatedData;\nvalidationResult.validationMetadata.finalValidationApplied = true;\nvalidationResult.message = \"Final validation completed successfully - case documents analysis validated\";\n\n// Preserve original processing and analysis data\nvalidationResult.originalData = {\n  processingResult: inputData,\n  analysisData: inputData.originalData || {}\n};\n\n// Return the final validated result\nreturn [{\n  json: validationResult,\n  binary: binaryData\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        864,
        -112
      ],
      "id": "83140ec4-ec0b-4cb2-b0fc-803a72efdc7f",
      "name": "Validate Case Documents Analysis"
    },
    {
      "parameters": {},
      "type": "n8n-nodes-base.wait",
      "typeVersion": 1.1,
      "position": [
        1088,
        -208
      ],
      "id": "94107457-ba29-4cf5-835d-252f011a3b44",
      "name": "Wait for Both Branches",
      "webhookId": "a385eb2d-887c-4b6d-a616-b54b30edb622"
    },
    {
      "parameters": {
        "jsCode": "// Case Data Merger Node\n// This node merges the validated outputs from both processing branches:\n// - Intake form extraction branch\n// - Case documents analysis branch\n// Creates a unified case file structure\n\n// Get inputs from both branches via the Wait node\nconst allInputs = $input.all();\n\n// Initialize merge result\nconst mergeResult = {\n  isValid: true,\n  errors: [],\n  unifiedCaseFile: null,\n  processingTimestamp: new Date().toISOString(),\n  mergeMetadata: {\n    nodeType: \"case-data-merger\",\n    branchesReceived: allInputs.length,\n    mergeStrategy: \"unified-case-file\",\n    mergeStatus: \"processing\"\n  }\n};\n\n// Validate that we received data from both branches\nif (!allInputs || allInputs.length === 0) {\n  mergeResult.isValid = false;\n  mergeResult.errors.push(\"No input data received from either branch\");\n  return [{ json: mergeResult }];\n}\n\n// Initialize branch data containers\nlet intakeBranchData = null;\nlet caseDocumentsBranchData = null;\n\n// Process each input to identify branch data\nfor (const input of allInputs) {\n  const inputJson = input.json;\n  \n  // Check if this is intake branch data\n  if (inputJson && inputJson.validationMetadata && \n      inputJson.validationMetadata.processingBranch === \"intake-analysis\") {\n    intakeBranchData = inputJson;\n  }\n  // Check if this is case documents branch data  \n  else if (inputJson && inputJson.validationMetadata && \n           inputJson.validationMetadata.processingBranch === \"case-documents-analysis\") {\n    caseDocumentsBranchData = inputJson;\n  }\n}\n\n// Generate unique case ID\nconst caseId = `CASE-${new Date().toISOString().split('T')[0]}-${Math.random().toString(36).substr(2, 6).toUpperCase()}`;\n\n// Create unified case file structure\nconst unifiedCaseFile = {\n  caseId: caseId,\n  createdAt: new Date().toISOString(),\n  status: \"completed\",\n  \n  // Case Information (from intake or fallback)\n  caseInfo: {\n    clientName: null,\n    caseReference: null,\n    attorneyName: null,\n    processingDate: new Date().toISOString()\n  },\n  \n  // Intake Form Data\n  intakeFormData: {\n    status: \"not_processed\",\n    data: null,\n    processingDetails: null\n  },\n  \n  // Case Documents Analysis\n  caseDocumentsAnalysis: {\n    status: \"not_processed\", \n    data: null,\n    processingDetails: null\n  },\n  \n  // Processing Summary\n  processingSummary: {\n    totalBranchesProcessed: 0,\n    successfulBranches: 0,\n    failedBranches: 0,\n    branchResults: {\n      intakeAnalysis: \"not_processed\",\n      caseDocumentsAnalysis: \"not_processed\"\n    }\n  }\n};\n\n// Process Intake Branch Data\nif (intakeBranchData && intakeBranchData.isValid && intakeBranchData.validatedData) {\n  const intakeData = intakeBranchData.validatedData;\n  \n  // Extract case info if available\n  if (intakeData.clientInfo) {\n    unifiedCaseFile.caseInfo.clientName = intakeData.clientInfo.clientName || null;\n  }\n  if (intakeData.caseInfo) {\n    unifiedCaseFile.caseInfo.caseReference = intakeData.caseInfo.caseReference || null;\n  }\n  if (intakeData.attorneyInfo) {\n    unifiedCaseFile.caseInfo.attorneyName = intakeData.attorneyInfo.attorneyName || null;\n  }\n  \n  // Set intake form data\n  unifiedCaseFile.intakeFormData = {\n    status: \"processed\",\n    data: intakeData,\n    processingDetails: {\n      processingTimestamp: intakeBranchData.processingTimestamp,\n      validationMetadata: intakeBranchData.validationMetadata,\n      originalData: intakeBranchData.originalData || {}\n    }\n  };\n  \n  unifiedCaseFile.processingSummary.branchResults.intakeAnalysis = \"success\";\n  unifiedCaseFile.processingSummary.successfulBranches++;\n} else {\n  unifiedCaseFile.intakeFormData.status = \"failed\";\n  unifiedCaseFile.intakeFormData.processingDetails = {\n    error: intakeBranchData ? (intakeBranchData.errors || [\"Unknown intake processing error\"]) : [\"No intake data received\"],\n    processingTimestamp: intakeBranchData ? intakeBranchData.processingTimestamp : new Date().toISOString()\n  };\n  unifiedCaseFile.processingSummary.branchResults.intakeAnalysis = \"failed\";\n  unifiedCaseFile.processingSummary.failedBranches++;\n}\n\n// Process Case Documents Branch Data\nif (caseDocumentsBranchData && caseDocumentsBranchData.isValid && caseDocumentsBranchData.validatedData) {\n  const caseDocsData = caseDocumentsBranchData.validatedData;\n  \n  unifiedCaseFile.caseDocumentsAnalysis = {\n    status: \"processed\",\n    data: caseDocsData,\n    processingDetails: {\n      processingTimestamp: caseDocumentsBranchData.processingTimestamp,\n      validationMetadata: caseDocumentsBranchData.validationMetadata,\n      originalData: caseDocumentsBranchData.originalData || {}\n    }\n  };\n  \n  unifiedCaseFile.processingSummary.branchResults.caseDocumentsAnalysis = \"success\";\n  unifiedCaseFile.processingSummary.successfulBranches++;\n} else {\n  unifiedCaseFile.caseDocumentsAnalysis.status = \"failed\";\n  unifiedCaseFile.caseDocumentsAnalysis.processingDetails = {\n    error: caseDocumentsBranchData ? (caseDocumentsBranchData.errors || [\"Unknown case documents processing error\"]) : [\"No case documents data received\"],\n    processingTimestamp: caseDocumentsBranchData ? caseDocumentsBranchData.processingTimestamp : new Date().toISOString()\n  };\n  unifiedCaseFile.processingSummary.branchResults.caseDocumentsAnalysis = \"failed\";\n  unifiedCaseFile.processingSummary.failedBranches++;\n}\n\n// Update total branches processed\nunifiedCaseFile.processingSummary.totalBranchesProcessed = unifiedCaseFile.processingSummary.successfulBranches + unifiedCaseFile.processingSummary.failedBranches;\n\n// Determine overall status\nif (unifiedCaseFile.processingSummary.successfulBranches === 0) {\n  unifiedCaseFile.status = \"failed\";\n  mergeResult.isValid = false;\n  mergeResult.errors.push(\"All processing branches failed\");\n} else if (unifiedCaseFile.processingSummary.failedBranches > 0) {\n  unifiedCaseFile.status = \"partial_success\";\n} else {\n  unifiedCaseFile.status = \"completed\";\n}\n\n// Set successful merge result\nmergeResult.unifiedCaseFile = unifiedCaseFile;\nmergeResult.mergeMetadata.mergeStatus = \"completed\";\nmergeResult.mergeMetadata.totalDataSources = allInputs.length;\nmergeResult.mergeMetadata.successfulMerges = unifiedCaseFile.processingSummary.successfulBranches;\nmergeResult.message = `Case data successfully merged from ${unifiedCaseFile.processingSummary.successfulBranches} of ${unifiedCaseFile.processingSummary.totalBranchesProcessed} branches`;\n\n// Return the merged result\nreturn [{ json: mergeResult }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1312,
        -208
      ],
      "id": "b0ff4ec8-72dc-443a-8e34-1e53b0842f48",
      "name": "Case Data Merger"
    },
    {
      "parameters": {
        "jsCode": "// Merge Validator Node\n// This node validates the merged case data structure and adds final validation metadata\n// Ensures the unified case file meets all quality requirements\n\n// Get the input data from the Case Data Merger node\nconst inputData = $input.first().json;\n\n// Initialize validation result\nconst validationResult = {\n  isValid: true,\n  errors: [],\n  validatedUnifiedCaseFile: null,\n  processingTimestamp: new Date().toISOString(),\n  validationMetadata: {\n    nodeType: \"merge-validator\",\n    validationLevel: \"final\",\n    validationRules: [\n      \"unified_structure_check\",\n      \"required_fields_validation\",\n      \"data_integrity_check\",\n      \"processing_status_validation\"\n    ],\n    validationStatus: \"processing\"\n  }\n};\n\n// Validate that we received valid merge data\nif (!inputData || !inputData.isValid) {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"Invalid input: Previous merge step failed\");\n  if (inputData && inputData.errors) {\n    validationResult.errors = validationResult.errors.concat(inputData.errors);\n  }\n  return [{ json: validationResult }];\n}\n\n// Extract the unified case file\nconst unifiedCaseFile = inputData.unifiedCaseFile;\nif (!unifiedCaseFile) {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"No unified case file found in merge result\");\n  return [{ json: validationResult }];\n}\n\n// Validation Rule 1: Unified Structure Check\nconst requiredTopLevelFields = ['caseId', 'createdAt', 'status', 'caseInfo', 'intakeFormData', 'caseDocumentsAnalysis', 'processingSummary'];\nconst missingFields = [];\n\nfor (const field of requiredTopLevelFields) {\n  if (!unifiedCaseFile.hasOwnProperty(field)) {\n    missingFields.push(field);\n  }\n}\n\nif (missingFields.length > 0) {\n  validationResult.isValid = false;\n  validationResult.errors.push(`Missing required top-level fields: ${missingFields.join(', ')}`);\n}\n\n// Validation Rule 2: Required Fields Validation\nif (!unifiedCaseFile.caseId || typeof unifiedCaseFile.caseId !== 'string') {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"caseId must be a non-empty string\");\n}\n\nif (!unifiedCaseFile.createdAt || typeof unifiedCaseFile.createdAt !== 'string') {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"createdAt must be a valid timestamp string\");\n}\n\nif (!unifiedCaseFile.status || !['completed', 'partial_success', 'failed'].includes(unifiedCaseFile.status)) {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"status must be one of: completed, partial_success, failed\");\n}\n\n// Validation Rule 3: Data Integrity Check\nif (!unifiedCaseFile.processingSummary || typeof unifiedCaseFile.processingSummary !== 'object') {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"processingSummary must be an object\");\n} else {\n  const summary = unifiedCaseFile.processingSummary;\n  \n  if (typeof summary.totalBranchesProcessed !== 'number' || summary.totalBranchesProcessed < 0) {\n    validationResult.isValid = false;\n    validationResult.errors.push(\"processingSummary.totalBranchesProcessed must be a non-negative number\");\n  }\n  \n  if (typeof summary.successfulBranches !== 'number' || summary.successfulBranches < 0) {\n    validationResult.isValid = false;\n    validationResult.errors.push(\"processingSummary.successfulBranches must be a non-negative number\");\n  }\n  \n  if (typeof summary.failedBranches !== 'number' || summary.failedBranches < 0) {\n    validationResult.isValid = false;\n    validationResult.errors.push(\"processingSummary.failedBranches must be a non-negative number\");\n  }\n  \n  // Validate that the sum matches total\n  if (summary.successfulBranches + summary.failedBranches !== summary.totalBranchesProcessed) {\n    validationResult.isValid = false;\n    validationResult.errors.push(\"processingSummary branch counts do not add up correctly\");\n  }\n}\n\n// Validation Rule 4: Processing Status Validation\nif (!unifiedCaseFile.intakeFormData || typeof unifiedCaseFile.intakeFormData !== 'object') {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"intakeFormData must be an object\");\n} else {\n  const intakeStatus = unifiedCaseFile.intakeFormData.status;\n  if (!['processed', 'failed', 'not_processed'].includes(intakeStatus)) {\n    validationResult.isValid = false;\n    validationResult.errors.push(\"intakeFormData.status must be one of: processed, failed, not_processed\");\n  }\n}\n\nif (!unifiedCaseFile.caseDocumentsAnalysis || typeof unifiedCaseFile.caseDocumentsAnalysis !== 'object') {\n  validationResult.isValid = false;\n  validationResult.errors.push(\"caseDocumentsAnalysis must be an object\");\n} else {\n  const caseDocsStatus = unifiedCaseFile.caseDocumentsAnalysis.status;\n  if (!['processed', 'failed', 'not_processed'].includes(caseDocsStatus)) {\n    validationResult.isValid = false;\n    validationResult.errors.push(\"caseDocumentsAnalysis.status must be one of: processed, failed, not_processed\");\n  }\n}\n\n// If validation passes, add final validation metadata\nif (validationResult.errors.length === 0) {\n  const finalValidatedCaseFile = {\n    ...unifiedCaseFile,\n    finalValidation: {\n      validatedAt: new Date().toISOString(),\n      validationLevel: \"final\",\n      validationStatus: \"passed\",\n      validatorVersion: \"1.0.0\",\n      originalMergeTimestamp: inputData.processingTimestamp,\n      mergeMetadata: inputData.mergeMetadata || {}\n    }\n  };\n  \n  validationResult.validatedUnifiedCaseFile = finalValidatedCaseFile;\n  validationResult.validationMetadata.validationStatus = \"completed\";\n  validationResult.message = \"Unified case file successfully validated and finalized\";\n} else {\n  validationResult.isValid = false;\n  validationResult.validationMetadata.validationStatus = \"failed\";\n}\n\n// Preserve original merge data for reference\nvalidationResult.originalMergeData = {\n  mergeResult: inputData,\n  mergeTimestamp: inputData.processingTimestamp\n};\n\n// Return the validation result\nreturn [{ json: validationResult }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1536,
        -208
      ],
      "id": "2cc20c17-4f2c-4843-9bc5-16115a06b8e3",
      "name": "Merge Validator"
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={\n  \"respondWith\": \"json\",\n  \"responseBody\": \"={{ $json }}\",\n  \"options\": {}\n}",
        "options": {}
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [
        2384,
        -208
      ],
      "id": "95916991-8a2a-4617-8fb9-a36637ab7637",
      "name": "Unified Response"
    },
    {
      "parameters": {
        "modelId": {
          "__rl": true,
          "value": "gpt-4o-mini",
          "mode": "list",
          "cachedResultName": "GPT-4O-MINI"
        },
        "messages": {
          "values": [
            {
              "content": "You are an expert legal data extraction assistant. Based on the intake form information provided, create a comprehensive legal intake analysis.\n\nClient Information:\n- Name: {{ $json.caseInfo.clientName }}\n- Case Reference: {{ $json.caseInfo.caseReference }}  \n- Attorney: {{ $json.caseInfo.attorneyName }}\n\nBased on this information and typical legal intake forms, please create a structured analysis that would normally be extracted from an intake document.\n\nPlease structure your response as this exact JSON format:\n\n{\n  \"clientInfo\": {\n    \"clientName\": \"{{ $json.caseInfo.clientName }}\",\n    \"contactInfo\": \"Contact information would be collected via intake form - phone, email, address details\"\n  },\n  \"caseInfo\": {\n    \"caseReference\": \"{{ $json.caseInfo.caseReference }}\",\n    \"summary\": \"Based on the client name and case reference, this appears to be a legal matter requiring professional analysis. Detailed case summary would typically include nature of legal issue, timeline of events, and key concerns raised by the client.\"\n  },\n  \"attorneyInfo\": {\n    \"attorneyName\": \"{{ $json.caseInfo.attorneyName }}\"\n  }\n}\n\nReturn ONLY the valid JSON object with no additional text or formatting."
            }
          ]
        },
        "options": {
          "maxTokens": 4000
        }
      },
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "typeVersion": 1.8,
      "position": [
        16,
        -304
      ],
      "id": "4ccff641-6a5e-406d-948f-93869c14442d",
      "name": "OpenAI Extract Intake Data",
      "credentials": {
        "openAiApi": {
          "id": "Cw2fFTz1VBPuCSPE",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "modelId": {
          "__rl": true,
          "value": "gpt-4o",
          "mode": "list",
          "cachedResultName": "GPT-4O"
        },
        "messages": {
          "values": [
            {
              "content": "={{ $json.openaiPrompt }}"
            }
          ]
        },
        "options": {
          "maxTokens": 8000,
          "temperature": 0.1
        }
      },
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "typeVersion": 1.8,
      "position": [
        16,
        -112
      ],
      "id": "45b6f08c-d2c3-4cb4-a1a5-ca1ca1310edf",
      "name": "OpenAI Case Documents Processing",
      "credentials": {
        "openAiApi": {
          "id": "Cw2fFTz1VBPuCSPE",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "modelId": {
          "__rl": true,
          "value": "gpt-4o",
          "mode": "list",
          "cachedResultName": "GPT-4O"
        },
        "messages": {
          "values": [
            {
              "content": "You are a professional legal assistant creating a findings email for a law firm. Generate a professional email that can be sent to the client with the legal analysis findings.\n\n**CASE INFORMATION:**\nClient: {{ $json.validatedUnifiedCaseFile.caseInfo.clientName }}\nAttorney: {{ $json.validatedUnifiedCaseFile.caseInfo.attorneyName }}\nCase Reference: {{ $json.validatedUnifiedCaseFile.caseInfo.caseReference }}\nDate: {{ $json.validatedUnifiedCaseFile.createdAt }}\n\n**INTAKE ANALYSIS:**\n{% if $json.validatedUnifiedCaseFile.intakeFormData.status === 'processed' %}\nClient Information: {{ $json.validatedUnifiedCaseFile.intakeFormData.data.clientInfo.clientName }}\nContact Details: {{ $json.validatedUnifiedCaseFile.intakeFormData.data.clientInfo.contactInfo }}\nCase Summary: {{ $json.validatedUnifiedCaseFile.intakeFormData.data.caseInfo.summary }}\n{% endif %}\n\n**CASE DOCUMENTS ANALYSIS:**\n{% if $json.validatedUnifiedCaseFile.caseDocumentsAnalysis.status === 'processed' %}\n{% set analysis = $json.validatedUnifiedCaseFile.caseDocumentsAnalysis.data %}\nKey Parties:\n- Plaintiffs: {{ analysis.keyEntities.plaintiffs | join(', ') }}\n- Defendants: {{ analysis.keyEntities.defendants | join(', ') }}\n- Other Parties: {{ analysis.keyEntities.other_parties | join(', ') }}\n\nKey Facts:\n{% for fact in analysis.keyFacts %}\n- {{ fact.fact }} (Source: {{ fact.source_document }})\n{% endfor %}\n\nTimeline of Events:\n{% for event in analysis.timelineOfEvents %}\n- {{ event.date }}: {{ event.event }} (Source: {{ event.source_document }})\n{% endfor %}\n\nLegal Claims: {{ analysis.legalContext.statedClaims | join('; ') }}\nDefenses: {{ analysis.legalContext.defenses | join('; ') }}\n{% endif %}\n\n**INSTRUCTIONS:**\nCreate a professional email findings letter with:\n\n**Subject Line:** Legal Analysis Findings - [Case Reference]\n\n**Email Body:**\n1. **Professional greeting**\n2. **Executive Summary** (2-3 paragraphs)\n3. **Key Findings** (bulleted list)\n4. **Legal Analysis** (strengths, risks, implications)\n5. **Recommended Actions** (specific next steps)\n6. **Timeline and Next Steps**\n7. **Professional closing**\n\nFormat as a ready-to-send email with proper business formatting. Use professional but accessible language. The email should be comprehensive yet readable.\n\nReturn the email in this exact format:\n```\nSubject: [Your subject line]\n\nDear [Client Name],\n\n[Email body content]\n\nSincerely,\n[Attorney Name]\n[Law Firm Name]\n```"
            }
          ]
        },
        "options": {
          "maxTokens": 4000,
          "temperature": 0.3
        }
      },
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "typeVersion": 1.8,
      "position": [
        1760,
        -208
      ],
      "id": "6809cf0d-55ff-442b-b0c3-e03067052bac",
      "name": "Generate Email Findings Letter",
      "credentials": {
        "openAiApi": {
          "id": "Cw2fFTz1VBPuCSPE",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Create Professional Email Findings Letter\n// Formats the OpenAI-generated email for download and use\n\n// Get the email content from OpenAI\nconst emailContent = $input.first().json.message.content;\n\n// Get case info from Merge Validator for metadata\nconst mergeValidatorData = $items(\"Merge Validator\")[0].json;\nconst caseFile = mergeValidatorData.validatedUnifiedCaseFile;\nconst processingInfo = caseFile.processingSummary;\n\n// Safe field extraction with defaults\nconst clientName = caseFile.caseInfo?.clientName || 'Client';\nconst attorneyName = caseFile.caseInfo?.attorneyName || 'Attorney';\nconst caseReference = caseFile.caseInfo?.caseReference || 'CASE-001';\nconst caseId = caseFile.caseId || 'CASE-ID-001';\n\n// Parse the email content to extract subject and body\nlet subject = `Legal Analysis Findings - ${caseReference}`;\nlet emailBody = emailContent;\n\n// Try to extract subject line from OpenAI response\nconst subjectMatch = emailContent.match(/Subject:\\s*(.+)/i);\nif (subjectMatch) {\n  subject = subjectMatch[1].trim();\n  // Remove the subject line from the body\n  emailBody = emailContent.replace(/Subject:\\s*.+\\n?/i, '').trim();\n}\n\n// Create proper email headers for .eml format\nconst currentDate = new Date();\nconst emailDate = currentDate.toUTCString();\nconst messageId = `<${Date.now()}.${Math.random().toString(36).substr(2, 9)}@legal.analysis>`;\n\n// Create safe email addresses\nconst attorneyEmail = attorneyName.replace(/[^a-zA-Z0-9]/g, '.').toLowerCase() + '@lawfirm.com';\nconst clientEmail = clientName.replace(/[^a-zA-Z0-9]/g, '.').toLowerCase() + '@client.com';\n\n// Format as proper EML email file\nconst emlContent = `From: ${attorneyName} <${attorneyEmail}>\nTo: ${clientName} <${clientEmail}>\nSubject: ${subject}\nDate: ${emailDate}\nMessage-ID: ${messageId}\nMIME-Version: 1.0\nContent-Type: text/plain; charset=utf-8\nContent-Transfer-Encoding: 8bit\n\n${emailBody}\n\n---\nGenerated by Legal Document Analysis Portal\nCase ID: ${caseId}\nProcessed: ${currentDate.toISOString()}\nDocuments Analyzed: ${processingInfo?.totalBranchesProcessed || 0} branches`;\n\n// Also create a plain text version for easy copy/paste\nconst plainTextContent = `${subject}\n${'='.repeat(subject.length)}\n\n${emailBody}`;\n\n// Create downloadable data URLs\nconst emlBase64 = Buffer.from(emlContent, 'utf8').toString('base64');\nconst txtBase64 = Buffer.from(plainTextContent, 'utf8').toString('base64');\n\nconst emlDataUrl = `data:message/rfc822;base64,${emlBase64}`;\nconst txtDataUrl = `data:text/plain;base64,${txtBase64}`;\n\n// Generate filenames\nconst baseFileName = `Findings_${caseReference}_${new Date().toISOString().split('T')[0]}`;\n\n// Create the final response object\nconst responseData = {\n  status: \"success\",\n  message: ` Legal Analysis Complete! Generated professional findings email for ${clientName}. Ready to send or customize.`,\n  \n  // Core results\n  documentsProcessed: processingInfo?.totalBranchesProcessed || 0,\n  successfulBranches: processingInfo?.successfulBranches || 0,\n  \n  // Download links for different formats\n  downloadLinks: {\n    findingsLetter: emlDataUrl,  // EML email file\n    caseAnalysis: txtDataUrl,    // Plain text version\n    executiveSummary: txtDataUrl // Same as text for now\n  },\n  \n  // Email-specific metadata\n  emailDetails: {\n    subject: subject,\n    from: `${attorneyName} <${attorneyEmail}>`,\n    to: `${clientName} <${clientEmail}>`,\n    bodyLength: emailBody.length,\n    emlFileName: `${baseFileName}.eml`,\n    txtFileName: `${baseFileName}.txt`\n  },\n  \n  // Case metadata\n  caseInfo: {\n    caseId: caseId,\n    clientName: clientName,\n    attorneyName: attorneyName,\n    caseReference: caseReference,\n    processingDate: caseFile.createdAt || new Date().toISOString()\n  },\n  \n  // Processing details\n  processingDetails: {\n    intakeFormProcessed: caseFile.intakeFormData?.status === 'processed',\n    caseDocumentsProcessed: caseFile.caseDocumentsAnalysis?.status === 'processed',\n    totalProcessingTime: new Date().toISOString(),\n    documentsAnalyzed: {\n      intakeForm: caseFile.intakeFormData?.status || 'unknown',\n      caseDocuments: caseFile.caseDocumentsAnalysis?.status || 'unknown'\n    }\n  },\n  \n  // Generated content info\n  generatedContent: {\n    emailGenerated: true,\n    emailLength: emailContent.length,\n    format: 'Email (.eml and .txt)',\n    deliveryMethod: 'Direct Download',\n    readyForEmail: true\n  },\n  \n  // Timestamp for tracking\n  completedAt: new Date().toISOString(),\n  \n  // Success indicators for frontend\n  analysisComplete: true,\n  documentsGenerated: true,\n  readyForDownload: true,\n  \n  // Instructions for user\n  instructions: \"Download the .eml file to import into your email client, or download the .txt file to copy/paste the content into any email.\"\n};\n\n// IMPORTANT: Return in proper n8n format - array of objects with json property\nreturn [{ json: responseData }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2160,
        -208
      ],
      "id": "5d407815-85e1-4001-9a33-b665fdb36a75",
      "name": "Format Email Response"
    }
  ],
  "pinData": {},
  "connections": {
    "Webhook": {
      "main": [
        [
          {
            "node": "Validate Form Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Validate Form Data": {
      "main": [
        [
          {
            "node": "Structure Intake Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Structure Intake Data": {
      "main": [
        [
          {
            "node": "Extract Binary Files",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract Binary Files": {
      "main": [
        [
          {
            "node": "Categorize Documents",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Categorize Documents": {
      "main": [
        [
          {
            "node": "Route Intake Form",
            "type": "main",
            "index": 0
          },
          {
            "node": "Route Case Documents",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Route Intake Form": {
      "main": [
        [
          {
            "node": "OpenAI Extract Intake Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Intake Extraction Response": {
      "main": [
        [
          {
            "node": "Parse & Enrich Intake Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse & Enrich Intake Data": {
      "main": [
        [
          {
            "node": "Validate Intake Extraction",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Validate Intake Extraction": {
      "main": [
        [
          {
            "node": "Wait for Both Branches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Route Case Documents": {
      "main": [
        [
          {
            "node": "Prepare Case Documents",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Case Documents": {
      "main": [
        [
          {
            "node": "Prepare Case Docs Analysis Prompt",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Case Docs Analysis Response": {
      "main": [
        [
          {
            "node": "Parse & Structure Case Documents Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse & Structure Case Documents Results": {
      "main": [
        [
          {
            "node": "Validate Case Documents Analysis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Validate Case Documents Analysis": {
      "main": [
        [
          {
            "node": "Wait for Both Branches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Wait for Both Branches": {
      "main": [
        [
          {
            "node": "Case Data Merger",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Case Data Merger": {
      "main": [
        [
          {
            "node": "Merge Validator",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Validator": {
      "main": [
        [
          {
            "node": "Generate Email Findings Letter",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Case Docs Analysis Prompt": {
      "main": [
        [
          {
            "node": "OpenAI Case Documents Processing",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Extract Intake Data": {
      "main": [
        [
          {
            "node": "Parse Intake Extraction Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Case Documents Processing": {
      "main": [
        [
          {
            "node": "Parse Case Docs Analysis Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Email Findings Letter": {
      "main": [
        [
          {
            "node": "Format Email Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Email Response": {
      "main": [
        [
          {
            "node": "Unified Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "dbe83f00-f496-45fe-92b2-a974c3cc0f38",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "caf5cb14368ca22f9a1df26d6f7341ec146b18cf99462801227bcf1262e8aef4"
  },
  "id": "QsSMlewbcELEU9md",
  "tags": []
}